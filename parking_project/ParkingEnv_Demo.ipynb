{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ParkingEnv 交互式演示（中文说明）\n",
    "\n",
    "这个笔记本帮助你在 Jupyter 中配置并启动 `main.py` 或助力调参器，快速体验停车环境。\n",
    "\n",
    "- 先设置下一个代码单元中的运行参数（轮次数、最大步数、模式、动画速度）。\n",
    "- 修改参数后重新运行相关单元即可让 CLI 使用新的值。\n",
    "- `manual` 模式下，Qt 窗口使用方向键：`↑/↓` 控制油门，`←/→` 控制方向，`Esc` 退出。\n",
    "- 每个代码单元都会说明是否写入 JSON 配置，按需调整文件路径后执行即可。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4785e20e",
   "metadata": {},
   "source": [
    "# ParkingEnv Interactive Demo (English Notes)\n",
    "\n",
    "This notebook shows how to configure and launch `main.py` or the assist-model tuner straight from Jupyter.\n",
    "\n",
    "- Set the run parameters in the next code cell (episode count, max steps, mode, animation speed).\n",
    "- Rerun the relevant cells after changing a value so the CLI picks up the new settings.\n",
    "- In `manual` mode the Qt window listens to the arrow keys: `↑/↓` throttle, `←/→` steering, `Esc` to exit.\n",
    "- Each code cell tells you whether it writes a JSON config; adjust the file paths first if needed, then execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca995f54",
   "metadata": {},
   "source": [
    "## 运行参数设置（中文说明）\n",
    "\n",
    "下方代码块定义 CLI 演示所需的基础变量。\n",
    "\n",
    "- `episodes` 控制重复运行次数，通常 1 次即可快速检查界面。\n",
    "- `max_steps` 设定每轮的最大仿真步数，避免策略长时间卡住。\n",
    "- `sleep_scale` 调整动画节奏，值越大播放越慢，便于观察。\n",
    "- `mode` 选择 `manual` 或 `random`，也可以在下方保留备用模式以便切换。\n",
    "- 如需更多自定义项，可在运行后直接修改变量并重新执行。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70adc0c5",
   "metadata": {},
   "source": [
    "## Run Parameter Setup (English Notes)\n",
    "\n",
    "The next code cell declares the baseline variables used by the CLI demo.\n",
    "\n",
    "- `episodes` controls how many times the scenario repeats; a single run is usually enough to verify the UI.\n",
    "- `max_steps` caps the number of simulation steps per episode so random policies cannot run forever.\n",
    "- `sleep_scale` slows down the animation when set above `1.0`, which helps when you want to inspect behaviour frame by frame.\n",
    "- `mode` toggles between `manual` and `random`; keep an alternate assignment commented out for quick switching.\n",
    "- Feel free to add more parameters or tweak these values, then rerun the cell to update the downstream commands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81db3228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "episodes = 1\n",
    "max_steps = 4000\n",
    "sleep_scale = 0.5    # Larger values slow the animation\n",
    "\n",
    "mode = \"manual\"  # \"manual\" or \"random\"\n",
    "# mode = \"random\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f42a67",
   "metadata": {},
   "source": [
    "## 随机生成训练配置（中文说明）\n",
    "\n",
    "下方命令会调用 `generate_training_config.py`，只随机化车位和障碍布局，并将结果写入 `generated_configs/train_001.json`。\n",
    "\n",
    "- 执行前请先在外部终端激活 `parking-rl` Conda 环境，并确保依赖已经安装。\n",
    "- 如果不想覆盖现有文件，可修改 `--out` 路径或暂时注释这一行。\n",
    "- 想直接复用旧配置时可以跳过此单元格。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70601dfb",
   "metadata": {},
   "source": [
    "## Random Training Config Generator (English Notes)\n",
    "\n",
    "The next command runs `generate_training_config.py`, which randomizes only the parking-slot map parameters and writes them to `generated_configs/train_001.json`.\n",
    "\n",
    "- Activate the `parking-rl` Conda environment in an external terminal beforehand and ensure the dependencies are installed.\n",
    "- Edit or comment out the `--out` option if you prefer not to overwrite an existing file.\n",
    "- Skip this cell whenever you want to reuse a configuration that is already on disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5332545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# !conda activate parking-rl\n",
    "# !python generate_training_config.py --out generated_configs/train_001.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a98c73",
   "metadata": {},
   "source": [
    "## 指定配置文件路径（中文说明）\n",
    "\n",
    "这个单元负责控制是否写入 Notebook 生成的覆盖文件，以及 CLI 将加载哪份 JSON。\n",
    "\n",
    "- 将 `write_notebook_config` 设为 `True` 时，会把当前 `custom_config` 写回磁盘；默认 `False` 以保护现有文件。\n",
    "- 推荐默认指向 `generated_configs/notebook_override.json`，也可以改为训练配置或其他路径。\n",
    "- 调整 `config_path` 后请重新运行依赖该变量的后续单元。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0cfdf4",
   "metadata": {},
   "source": [
    "## Choose the Config File Path (English Notes)\n",
    "\n",
    "This code controls whether the notebook writes the override JSON and which file the CLI loads.\n",
    "\n",
    "- Set `write_notebook_config` to `True` when you want to persist `custom_config`; leave it `False` to avoid overwriting existing files.\n",
    "- The recommended default is `generated_configs/notebook_override.json`, but you can switch to the training config or any custom path.\n",
    "- After changing `config_path`, rerun the downstream cells that depend on it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f04f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_notebook_config = False  # Set to False to keep existing file contents\n",
    "config_path = Path(\"generated_configs/notebook_override.json\") # use default config for notebook\n",
    "# config_path = Path(\"generated_configs/train_001.json\") # use training config for notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe333259",
   "metadata": {},
   "source": [
    "## 载入默认配置副本（中文说明）\n",
    "\n",
    "以下代码会深拷贝 `build_config()` 返回的默认配置，并在目标 JSON 存在时读取它。\n",
    "\n",
    "- 如果 `config_path` 指向的文件存在，会优先加载其中的参数（保留 `__notes` 等附加信息）。\n",
    "- 若文件不存在，就使用默认配置并存入 `custom_config`，方便在 Notebook 中即时修改。\n",
    "- 可以利用末尾的示例查看或编辑任意字段，调整后配合写回逻辑即可保存。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df018bc3",
   "metadata": {},
   "source": [
    "## Load a Working Copy of the Config (English Notes)\n",
    "\n",
    "The next cell deep-copies the defaults returned by `build_config()` and reads the target JSON when it is available.\n",
    "\n",
    "- When `config_path` already exists, its contents are loaded first, including any extra fields such as `__notes`.\n",
    "- If the file is missing, the code falls back to the default dictionary and keeps it in `custom_config` for quick edits.\n",
    "- Use the examples at the end of the cell to inspect or update individual fields, then combine with the write-back logic to persist your changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1277a326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from main import build_config\n",
    "\n",
    "# Load existing notebook override (keeps __notes) when available; fall back to defaults otherwise.\n",
    "if config_path.exists():\n",
    "    with config_path.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "        custom_config = json.load(fh)\n",
    "else:\n",
    "    custom_config = deepcopy(build_config())\n",
    "\n",
    "# You can modify custom_config here if desired, e.g.:\n",
    "# custom_config # shows all config parameters, including defaults\n",
    "# custom_config[\"vehicle\"][\"length\"] # show specific parameter\n",
    "# custom_config[\"vehicle\"][\"length\"] = 4.5 # modify specific parameter\n",
    "# custom_config[\"vehicle\"][\"length\"] # verify change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b6b1e0",
   "metadata": {},
   "source": [
    "## 状态 / 动作 / 奖励 总览（中文说明）\n",
    "\n",
    "**状态空间**\n",
    "- 全局位置 `(x, y)`：车辆中心在世界坐标系中的位置，内部已按 `±field_size/2` 归一化；实际训练时可再做特征缩放。\n",
    "- `cos(yaw)` 与 `sin(yaw)`：车辆朝向的三角编码，避免角度取模问题，数值范围 [-1, 1]。\n",
    "- 纵向速度 `velocity`：受 `max_speed` 与 `max_reverse_speed` 限制，典型区间约 [-2.0, 3.0] m/s。\n",
    "- 转向角 `steering_angle`：裁剪在 `±max_steering_angle` 内，默认约 ±0.79 rad (±45°)。\n",
    "- 转向角速度 `steering_rate`：裁剪在 `±max_steering_rate` 内，默认约 ±1.05 rad/s (±60°/s)。\n",
    "- 车位坐标误差 `(slot_dx, slot_dy)`：车辆在车位坐标系下的平移偏差，采用场地大小归一化。\n",
    "- `cos(Δyaw)` 与 `sin(Δyaw)`：车身与车位朝向差的三角编码，范围 [-1, 1]。\n",
    "- 激光测距 `ray_1 … ray_N`：共 `N = len(ray_angles)` 束射线，取值 [0, 1]，0 表示立即碰撞，1 表示达到 `ray_max_range`。\n",
    "- 除非使用 `raw=True`，观测默认叠加标准差为 0.005 的高斯噪声，可通过 `ParkingEnv(config={\"observation_noise\": {\"enabled\": False}})` 或 `env.unwrapped.set_observation_noise(...)` 关闭/调整。\n",
    "\n",
    "**动作空间（Box(2))**\n",
    "- `a_longitudinal`：纵向加速度指令，范围 [-3.0, 2.0] m/s²，正值加速、负值制动或倒车。\n",
    "- `a_steering`：转向角加速度指令，范围 [-1.5, 1.5] rad/s²；在 `step()` 中会裁剪到上限以保持稳定。\n",
    "\n",
    "**奖励构成**\n",
    "- `distance`：`-distance_scale * ||slot_xy||`，鼓励尽快贴近车位中心。\n",
    "- `heading`：`-heading_scale * |Δyaw|`，惩罚朝向误差。\n",
    "- `velocity`：`-velocity_penalty * |v|`，在靠近车位时鼓励降速。\n",
    "- `smoothness`：`-smoothness * steering_rate^2`，约束方向盘抖动，与辅助模型阻尼相关。\n",
    "- `step`：每步扣除 `step_cost`，促使策略提高效率。\n",
    "- `collision`：碰撞时一次扣除 `collision`（默认 -120）。\n",
    "- `success`：满足位置、朝向与速度阈值时给予 `success` 奖励（默认 +140）。\n",
    "- 以上权重均来源于 `config['reward']`，可在 JSON 中调节以改变训练侧重点。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010288dd",
   "metadata": {},
   "source": [
    "## State / Action / Reward Overview (English Notes)\n",
    "\n",
    "**State space**\n",
    "- Global position `(x, y)`: vehicle centre in world coordinates, internally normalized by `±field_size/2`; you may apply additional feature scaling for training.\n",
    "- `cos(yaw)` and `sin(yaw)`: sine/cosine encoding of the vehicle heading to avoid wrap-around issues, each within [-1, 1].\n",
    "- Longitudinal velocity `velocity`: bounded by `max_speed` and `max_reverse_speed`, typically around [-2.0, 3.0] m/s.\n",
    "- Steering angle `steering_angle`: clipped to `±max_steering_angle`, roughly ±0.79 rad (±45°) by default.\n",
    "- Steering rate `steering_rate`: clipped to `±max_steering_rate`, roughly ±1.05 rad/s (±60°/s) by default.\n",
    "- Slot-frame error `(slot_dx, slot_dy)`: translation of the vehicle relative to the slot frame, normalized by the field size.\n",
    "- `cos(Δyaw)` and `sin(Δyaw)`: heading difference between the vehicle and the slot, encoded in [-1, 1].\n",
    "- Range readings `ray_1 … ray_N`: `N = len(ray_angles)` lidar beams scaled to [0, 1]; 0 means immediate collision, 1 reaches `ray_max_range`.\n",
    "- Unless you request `raw=True`, a Gaussian noise with std 0.005 is added; disable or tune it via `ParkingEnv(config={\"observation_noise\": {\"enabled\": False}})` or `env.unwrapped.set_observation_noise(...)`.\n",
    "\n",
    "**Action space (Box(2))**\n",
    "- `a_longitudinal`: longitudinal acceleration command in [-3.0, 2.0] m/s²; positive accelerates forward, negative brakes or reverses.\n",
    "- `a_steering`: steering angular acceleration command in [-1.5, 1.5] rad/s²; the environment clips it during `step()` for stability.\n",
    "\n",
    "**Reward components**\n",
    "- `distance`: `-distance_scale * ||slot_xy||`, encouraging the agent to reach the slot centre quickly.\n",
    "- `heading`: `-heading_scale * |Δyaw|`, penalising heading misalignment.\n",
    "- `velocity`: `-velocity_penalty * |v|`, encouraging the car to slow down near the slot.\n",
    "- `smoothness`: `-smoothness * steering_rate^2`, discouraging steering oscillations and linking to the assist-model damping.\n",
    "- `step`: subtracts `step_cost` every timestep to promote efficiency.\n",
    "- `collision`: applies `collision` once on impact (default -120).\n",
    "- `success`: grants `success` when position, heading, and velocity thresholds are met (default +140).\n",
    "- All weights originate from `config['reward']`, so adjust the JSON to shift training priorities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a473cf2",
   "metadata": {},
   "source": [
    "## 助力模型说明（中文说明）\n",
    "\n",
    "- 当方向盘角加速度指令满足 `|α_cmd| ≤ deadband` 时（即松开方向盘），系统触发回正模型。\n",
    "- 回正加速度 `α_assist = K_p · φ + K_d · φ̇`，并在积分阶段与驾驶员指令叠加更新方向盘角速度与角度。\n",
    "- 油门松开时，若纵向加速度满足 `|a_cmd| ≤ velocity_deadband`，则阻尼 `a_assist = K_v · v` 让车辆速度指数衰减。\n",
    "- 下方代码提供调参器的初始角度、角速度、仿真步数以及是否实时写回 JSON 的选项，便于快速尝试不同阻尼参数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401ad49a",
   "metadata": {},
   "source": [
    "## Assist-Model Notes (English Explanation)\n",
    "\n",
    "- When the steering angular acceleration command satisfies `|α_cmd| ≤ deadband` (driver releases the wheel), the assist model recenters it.\n",
    "- The assist acceleration follows `α_assist = K_p · φ + K_d · φ̇` and combines with the command during integration to update steering rate and angle.\n",
    "- When the throttle command magnitude stays within `velocity_deadband`, the damping term `a_assist = K_v · v` makes the velocity decay exponentially.\n",
    "- The next code cell sets the tuner defaults: initial steering angle/rate, simulation steps, and whether live edits sync back into the JSON file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd6af483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assist-model tuner launch options\n",
    "tuner_angle0 = 20.0   # 初始方向盘角度（度）\n",
    "tuner_rate0 = 0.0    # 初始方向盘角速度（度/秒）\n",
    "tuner_steps = 100     # 仿真步数\n",
    "tuner_sync_updates = True  # True 表示实时写回 JSON 配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5110eabc",
   "metadata": {},
   "source": [
    "## 启动助力调参器（中文说明）\n",
    "\n",
    "下面的命令会调用 `assist_model_tuner.py`，在 Qt 窗口中展示方向盘与油门阻尼的仿真结果。你可以滑动滑块来调节来调整阻尼参数，关闭窗口后自动保存。\n",
    "\n",
    "- 使用 `conda run -n parking-rl` 以 notebook 外部环境启动脚本，确保 Matplotlib 后端配置为 `QtAgg`。\n",
    "- 当 `tuner_sync_updates = True` 时，会把当前 `custom_config` 写入 JSON 并作为 `--config` 传入，让调参器实时读取。\n",
    "- 需要额外参数（例如时间步长）时，可在列表中继续追加命令行选项。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a07543f",
   "metadata": {},
   "source": [
    "## Launch the Assist Tuner (English Notes)\n",
    "\n",
    "The upcoming command runs `assist_model_tuner.py` and opens a Qt window that visualises the steering/throttle damping response. You can adjust the damping parameter by sliding the slider and save it automatically after closing the window.\n",
    "\n",
    "- It relies on `conda run -n parking-rl` so that the external environment provides the correct dependencies and the Matplotlib backend is set to `QtAgg`.\n",
    "- When `tuner_sync_updates = True`, the notebook writes `custom_config` to JSON and passes it via `--config`, letting the tuner pick up live edits.\n",
    "- Add further command-line options to `tuner_cmd` if you need extra knobs such as timestep or logging verbosity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "00e2aa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching assist model tuner: conda run -n parking-rl python assist_model_tuner.py --angle0 20.0 --rate0 0.0 --steps 100 --sync --config /home/ansatz/ME5418/parking_project/generated_configs/notebook_override.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "project_root = Path(\".\").resolve()\n",
    "tuner_cmd = [\n",
    "    \"conda\", \"run\", \"-n\", \"parking-rl\", \"python\", \"assist_model_tuner.py\",\n",
    "    \"--angle0\", str(tuner_angle0),\n",
    "    \"--rate0\", str(tuner_rate0),\n",
    "    \"--steps\", str(tuner_steps),\n",
    "]\n",
    "\n",
    "if tuner_sync_updates:\n",
    "    tuner_cmd.append(\"--sync\")\n",
    "\n",
    "if config_path is not None:\n",
    "    if write_notebook_config:\n",
    "        config_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with config_path.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "            json.dump(custom_config, fh, indent=2, ensure_ascii=False)\n",
    "            fh.write(\"\\n\")\n",
    "    tuner_cmd.extend([\"--config\", str(config_path.resolve())])\n",
    "\n",
    "print(\"Launching assist model tuner:\", \" \".join(tuner_cmd))\n",
    "env_vars = os.environ.copy()\n",
    "env_vars[\"MPLBACKEND\"] = \"QtAgg\"\n",
    "# subprocess.run(tuner_cmd, cwd=project_root, env=env_vars, check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61601c78",
   "metadata": {},
   "source": [
    "## 查看调参结果（中文说明）\n",
    "\n",
    "调参器运行后，可以执行下方代码读取当前配置文件，并打印关键阻尼参数以供核对。\n",
    "\n",
    "- 若 `write_notebook_config` 为 True，则会从刚刚写入的 JSON 读取数据；否则直接读取现有文件。\n",
    "- 输出的字段包括方向盘比例/阻尼、死区以及油门阻尼，便于确认是否符合预期。\n",
    "- 想检查更多字段时，可继续访问 `custom_config` 中的其他键值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9bebe",
   "metadata": {},
   "source": [
    "## Inspect Tuner Outputs (English Notes)\n",
    "\n",
    "After the tuner window closes, the next cell reloads the config file and prints the key damping parameters for verification.\n",
    "\n",
    "- When `write_notebook_config` is True, it reads the JSON that was just written; otherwise it loads the existing file on disk.\n",
    "- The printed values cover the steering proportional/derivative gains, deadband, and throttle damping so you can confirm the latest edits.\n",
    "- Feel free to probe additional entries inside `custom_config` if you need more diagnostics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e12c9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steering_damping Kp: 52.50000000000001\n",
      "steering_rate_damping Kd: 8.75\n",
      "steering_deadband: 0.03\n",
      "velocity_damping Kv: 2.45\n",
      "velocity_deadband: 0.03\n"
     ]
    }
   ],
   "source": [
    "# 调参可视化会在外部 Qt 窗口中渲染，运行上一个单元格即可启动。\n",
    "custom_config = json.load(config_path.open(\"r\", encoding=\"utf-8\"))\n",
    "vehicle_section = custom_config.get(\"vehicle\", {})\n",
    "print(\"steering_damping Kp:\", vehicle_section.get(\"steering_damping\"))  # 输出当前转向比例系数\n",
    "print(\"steering_rate_damping Kd:\", vehicle_section.get(\"steering_rate_damping\"))  # 输出当前转向速度阻尼\n",
    "print(\"steering_deadband:\", vehicle_section.get(\"steering_assist_deadband\"))  # 方向盘回正死区\n",
    "print(\"velocity_damping Kv:\", vehicle_section.get(\"velocity_damping\"))  # 油门释放阻尼\n",
    "print(\"velocity_deadband:\", vehicle_section.get(\"velocity_deadband\"))  # 油门释放死区"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e80f2",
   "metadata": {},
   "source": [
    "## 启动 CLI 演示（中文说明）\n",
    "\n",
    "接下来的代码将生成命令并运行 `main.py`，在 Qt 窗口中展示手动或随机策略。\n",
    "\n",
    "- 运行前请确认已经在外部终端激活 `parking-rl` 环境，确保依赖和 GUI 后端可用。\n",
    "- 如果勾选写入选项，会把当前 `custom_config` 保存成 JSON 并随命令加载。\n",
    "- 想修改睡眠倍速、轮次数等参数时，返回上方变量单元重新运行即可生效。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd74df14",
   "metadata": {},
   "source": [
    "## Launch the CLI Demo (English Notes)\n",
    "\n",
    "The next cell assembles the command for `main.py` and opens a Qt window to run either the manual or random policy.\n",
    "\n",
    "- Make sure the `parking-rl` environment is activated in an external terminal so that dependencies and the GUI backend are available.\n",
    "- When the write flag is enabled, the current `custom_config` is saved to JSON and passed to the CLI via `--config`.\n",
    "- To adjust sleep scaling, episode count, or any other parameter, go back to the earlier variable cell, tweak the values, and rerun it before launching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fb66c6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching CLI demo: conda run -n parking-rl python main.py --mode manual --episodes 1 --max-steps 4000 --config /home/ansatz/ME5418/parking_project/generated_configs/notebook_override.json\n",
      "Episode 1 Step 1 Reward -13.193 Termination running Distance 7.91 Heading 129.1 deg\n",
      "Episode 1 Step 2 Reward -13.189 Termination running Distance 7.91 Heading 129.1 deg\n",
      "Episode 1 Step 3 Reward -13.206 Termination running Distance 7.91 Heading 129.1 deg\n",
      "Episode 1 Step 4 Reward -13.209 Termination running Distance 7.91 Heading 129.1 deg\n",
      "Episode 1 Step 5 Reward -13.197 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 6 Reward -13.189 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 7 Reward -13.186 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 8 Reward -13.194 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 9 Reward -13.190 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 10 Reward -13.188 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 11 Reward -13.179 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 12 Reward -13.175 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 13 Reward -13.176 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 14 Reward -13.175 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 15 Reward -13.172 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 16 Reward -13.174 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 17 Reward -13.172 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 18 Reward -13.177 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 19 Reward -13.176 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 20 Reward -13.183 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 21 Reward -13.175 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 22 Reward -13.183 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 23 Reward -13.179 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 24 Reward -13.185 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 25 Reward -13.173 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 26 Reward -13.177 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 27 Reward -13.174 Termination running Distance 7.89 Heading 129.1 deg\n",
      "Episode 1 Step 28 Reward -13.168 Termination running Distance 7.89 Heading 129.1 deg\n",
      "Episode 1 Step 29 Reward -13.172 Termination running Distance 7.89 Heading 129.1 deg\n",
      "Episode 1 Step 30 Reward -13.170 Termination running Distance 7.89 Heading 129.1 deg\n",
      "Episode 1 Step 31 Reward -13.174 Termination running Distance 7.89 Heading 129.1 deg\n",
      "Episode 1 Step 32 Reward -13.174 Termination running Distance 7.89 Heading 129.1 deg\n",
      "Episode 1 Step 33 Reward -13.255 Termination running Distance 7.90 Heading 129.1 deg\n",
      "Episode 1 Step 34 Reward -13.347 Termination running Distance 7.92 Heading 129.1 deg\n",
      "Episode 1 Step 35 Reward -13.453 Termination running Distance 7.94 Heading 129.1 deg\n",
      "Episode 1 Step 36 Reward -13.566 Termination running Distance 7.98 Heading 129.1 deg\n",
      "Episode 1 Step 37 Reward -13.690 Termination running Distance 8.02 Heading 129.1 deg\n",
      "Episode 1 Step 38 Reward -13.827 Termination running Distance 8.07 Heading 129.1 deg\n",
      "Episode 1 Step 39 Reward -13.972 Termination running Distance 8.13 Heading 129.1 deg\n",
      "Episode 1 Step 40 Reward -13.921 Termination running Distance 8.18 Heading 129.1 deg\n",
      "Episode 1 Step 41 Reward -13.895 Termination running Distance 8.21 Heading 129.1 deg\n",
      "Episode 1 Step 42 Reward -13.872 Termination running Distance 8.24 Heading 129.1 deg\n",
      "Episode 1 Step 43 Reward -13.862 Termination running Distance 8.26 Heading 129.1 deg\n",
      "Episode 1 Step 44 Reward -13.849 Termination running Distance 8.28 Heading 129.1 deg\n",
      "Episode 1 Step 45 Reward -13.821 Termination running Distance 8.29 Heading 129.1 deg\n",
      "Episode 1 Step 46 Reward -13.780 Termination running Distance 8.29 Heading 129.1 deg\n",
      "Episode 1 Step 47 Reward -13.785 Termination running Distance 8.29 Heading 129.1 deg\n",
      "Episode 1 Step 48 Reward -13.814 Termination running Distance 8.27 Heading 129.1 deg\n",
      "Episode 1 Step 49 Reward -13.832 Termination running Distance 8.26 Heading 129.1 deg\n",
      "Episode 1 Step 50 Reward -13.849 Termination running Distance 8.23 Heading 129.1 deg\n",
      "Episode 1 Step 51 Reward -13.829 Termination running Distance 8.20 Heading 129.1 deg\n",
      "Episode 1 Step 52 Reward -13.811 Termination running Distance 8.17 Heading 129.1 deg\n",
      "Episode 1 Step 53 Reward -13.789 Termination running Distance 8.13 Heading 129.1 deg\n",
      "Episode 1 Step 54 Reward -13.773 Termination running Distance 8.08 Heading 129.1 deg\n",
      "Episode 1 Step 55 Reward -13.748 Termination running Distance 8.04 Heading 129.1 deg\n",
      "Episode 1 Step 56 Reward -13.716 Termination running Distance 7.98 Heading 129.1 deg\n",
      "Episode 1 Step 57 Reward -13.683 Termination running Distance 7.93 Heading 129.0 deg\n",
      "Episode 1 Step 58 Reward -13.641 Termination running Distance 7.87 Heading 129.0 deg\n",
      "Episode 1 Step 59 Reward -13.601 Termination running Distance 7.81 Heading 129.1 deg\n",
      "Episode 1 Step 60 Reward -13.553 Termination running Distance 7.75 Heading 129.1 deg\n",
      "Episode 1 Step 61 Reward -13.508 Termination running Distance 7.68 Heading 129.1 deg\n",
      "Episode 1 Step 62 Reward -13.470 Termination running Distance 7.62 Heading 129.2 deg\n",
      "Episode 1 Step 63 Reward -13.440 Termination running Distance 7.57 Heading 129.1 deg\n",
      "Episode 1 Step 64 Reward -13.406 Termination running Distance 7.52 Heading 129.1 deg\n",
      "Episode 1 Step 65 Reward -13.388 Termination running Distance 7.47 Heading 129.1 deg\n",
      "Episode 1 Step 66 Reward -13.382 Termination running Distance 7.44 Heading 128.9 deg\n",
      "Episode 1 Step 67 Reward -13.367 Termination running Distance 7.43 Heading 128.6 deg\n",
      "Episode 1 Step 68 Reward -13.360 Termination running Distance 7.43 Heading 128.3 deg\n",
      "Episode 1 Step 69 Reward -13.386 Termination running Distance 7.44 Heading 127.8 deg\n",
      "Episode 1 Step 70 Reward -13.420 Termination running Distance 7.48 Heading 127.2 deg\n",
      "Episode 1 Step 71 Reward -13.490 Termination running Distance 7.53 Heading 126.5 deg\n",
      "Episode 1 Step 72 Reward -13.588 Termination running Distance 7.59 Heading 125.7 deg\n",
      "Episode 1 Step 73 Reward -13.703 Termination running Distance 7.68 Heading 124.6 deg\n",
      "Episode 1 Step 74 Reward -13.850 Termination running Distance 7.79 Heading 123.4 deg\n",
      "Episode 1 Step 75 Reward -14.031 Termination running Distance 7.91 Heading 122.1 deg\n",
      "Episode 1 Step 76 Reward -14.240 Termination running Distance 8.06 Heading 120.5 deg\n",
      "Episode 1 Step 77 Reward -14.469 Termination running Distance 8.22 Heading 118.7 deg\n",
      "Episode 1 Step 78 Reward -14.732 Termination running Distance 8.40 Heading 116.6 deg\n",
      "Episode 1 Step 79 Reward -15.018 Termination running Distance 8.61 Heading 114.4 deg\n",
      "Episode 1 Step 80 Reward -15.322 Termination running Distance 8.83 Heading 111.8 deg\n",
      "Episode 1 Step 81 Reward -15.351 Termination running Distance 9.00 Heading 110.3 deg\n",
      "Episode 1 Step 82 Reward -15.386 Termination running Distance 9.13 Heading 109.3 deg\n",
      "Episode 1 Step 83 Reward -15.407 Termination running Distance 9.23 Heading 108.7 deg\n",
      "Episode 1 Step 84 Reward -15.429 Termination running Distance 9.30 Heading 108.4 deg\n",
      "Episode 1 Step 85 Reward -15.439 Termination running Distance 9.36 Heading 108.2 deg\n",
      "Episode 1 Step 86 Reward -15.445 Termination running Distance 9.41 Heading 108.2 deg\n",
      "Episode 1 Step 87 Reward -15.410 Termination running Distance 9.44 Heading 108.2 deg\n",
      "Episode 1 Step 88 Reward -15.362 Termination running Distance 9.45 Heading 108.2 deg\n",
      "Episode 1 Step 89 Reward -15.327 Termination running Distance 9.44 Heading 108.2 deg\n",
      "Episode 1 Step 90 Reward -15.365 Termination running Distance 9.43 Heading 108.2 deg\n",
      "Episode 1 Step 91 Reward -15.370 Termination running Distance 9.39 Heading 108.2 deg\n",
      "Episode 1 Step 92 Reward -15.360 Termination running Distance 9.34 Heading 108.2 deg\n",
      "Episode 1 Step 93 Reward -15.321 Termination running Distance 9.28 Heading 108.2 deg\n",
      "Episode 1 Step 94 Reward -15.269 Termination running Distance 9.21 Heading 108.2 deg\n",
      "Episode 1 Step 95 Reward -15.194 Termination running Distance 9.12 Heading 108.2 deg\n",
      "Episode 1 Step 96 Reward -15.098 Termination running Distance 9.02 Heading 108.2 deg\n",
      "Episode 1 Step 97 Reward -14.993 Termination running Distance 8.90 Heading 108.2 deg\n",
      "Episode 1 Step 98 Reward -14.860 Termination running Distance 8.78 Heading 108.2 deg\n",
      "Episode 1 Step 99 Reward -14.709 Termination running Distance 8.64 Heading 108.2 deg\n",
      "Episode 1 Step 100 Reward -14.509 Termination running Distance 8.51 Heading 108.3 deg\n",
      "Episode 1 Step 101 Reward -14.309 Termination running Distance 8.38 Heading 108.4 deg\n",
      "Episode 1 Step 102 Reward -14.128 Termination running Distance 8.25 Heading 108.5 deg\n",
      "Episode 1 Step 103 Reward -13.942 Termination running Distance 8.13 Heading 108.7 deg\n",
      "Episode 1 Step 104 Reward -13.765 Termination running Distance 8.00 Heading 108.9 deg\n",
      "Episode 1 Step 105 Reward -13.589 Termination running Distance 7.88 Heading 109.3 deg\n",
      "Episode 1 Step 106 Reward -13.413 Termination running Distance 7.77 Heading 109.7 deg\n",
      "Episode 1 Step 107 Reward -13.147 Termination running Distance 7.68 Heading 109.8 deg\n",
      "Episode 1 Step 108 Reward -12.946 Termination running Distance 7.62 Heading 109.9 deg\n",
      "Episode 1 Step 109 Reward -12.785 Termination running Distance 7.58 Heading 109.9 deg\n",
      "Episode 1 Step 110 Reward -12.685 Termination running Distance 7.54 Heading 109.9 deg\n",
      "Episode 1 Step 111 Reward -12.603 Termination running Distance 7.52 Heading 109.9 deg\n",
      "Episode 1 Step 112 Reward -12.528 Termination running Distance 7.50 Heading 109.9 deg\n",
      "Episode 1 Step 113 Reward -12.467 Termination running Distance 7.49 Heading 109.9 deg\n",
      "Episode 1 Step 114 Reward -12.412 Termination running Distance 7.48 Heading 109.9 deg\n",
      "Episode 1 Step 115 Reward -12.392 Termination running Distance 7.48 Heading 109.9 deg\n",
      "Episode 1 Step 116 Reward -12.446 Termination running Distance 7.49 Heading 109.9 deg\n",
      "Episode 1 Step 117 Reward -12.512 Termination running Distance 7.51 Heading 109.9 deg\n",
      "Episode 1 Step 118 Reward -12.585 Termination running Distance 7.53 Heading 109.8 deg\n",
      "Episode 1 Step 119 Reward -12.664 Termination running Distance 7.56 Heading 109.8 deg\n",
      "Episode 1 Step 120 Reward -12.778 Termination running Distance 7.60 Heading 109.7 deg\n",
      "Episode 1 Step 121 Reward -12.905 Termination running Distance 7.64 Heading 109.6 deg\n",
      "Episode 1 Step 122 Reward -13.027 Termination running Distance 7.70 Heading 109.4 deg\n",
      "Episode 1 Step 123 Reward -13.178 Termination running Distance 7.77 Heading 109.1 deg\n",
      "Episode 1 Step 124 Reward -13.325 Termination running Distance 7.85 Heading 108.7 deg\n",
      "Episode 1 Step 125 Reward -13.500 Termination running Distance 7.95 Heading 108.3 deg\n",
      "Episode 1 Step 126 Reward -13.703 Termination running Distance 8.05 Heading 107.6 deg\n",
      "Episode 1 Step 127 Reward -13.915 Termination running Distance 8.17 Heading 106.9 deg\n",
      "Episode 1 Step 128 Reward -14.162 Termination running Distance 8.30 Heading 105.9 deg\n",
      "Episode 1 Step 129 Reward -14.431 Termination running Distance 8.46 Heading 104.7 deg\n",
      "Episode 1 Step 130 Reward -14.731 Termination running Distance 8.63 Heading 103.3 deg\n",
      "Episode 1 Step 131 Reward -15.052 Termination running Distance 8.83 Heading 101.6 deg\n",
      "Episode 1 Step 132 Reward -15.405 Termination running Distance 9.04 Heading 99.6 deg\n",
      "Episode 1 Step 133 Reward -15.793 Termination running Distance 9.28 Heading 97.2 deg\n",
      "Episode 1 Step 134 Reward -16.210 Termination running Distance 9.54 Heading 94.4 deg\n",
      "Episode 1 Step 135 Reward -16.275 Termination running Distance 9.74 Heading 92.6 deg\n",
      "Episode 1 Step 136 Reward -16.327 Termination running Distance 9.89 Heading 91.4 deg\n",
      "Episode 1 Step 137 Reward -16.365 Termination running Distance 10.00 Heading 90.8 deg\n",
      "Episode 1 Step 138 Reward -16.401 Termination running Distance 10.09 Heading 90.4 deg\n",
      "Episode 1 Step 139 Reward -16.429 Termination running Distance 10.15 Heading 90.1 deg\n",
      "Episode 1 Step 140 Reward -16.435 Termination running Distance 10.19 Heading 90.0 deg\n",
      "Episode 1 Step 141 Reward -16.418 Termination running Distance 10.22 Heading 90.0 deg\n",
      "Episode 1 Step 142 Reward -16.364 Termination running Distance 10.23 Heading 90.0 deg\n",
      "Episode 1 Step 143 Reward -16.352 Termination running Distance 10.22 Heading 90.0 deg\n",
      "Episode 1 Step 144 Reward -16.372 Termination running Distance 10.19 Heading 90.0 deg\n",
      "Episode 1 Step 145 Reward -16.358 Termination running Distance 10.14 Heading 90.0 deg\n",
      "Episode 1 Step 146 Reward -16.329 Termination running Distance 10.08 Heading 90.0 deg\n",
      "Episode 1 Step 147 Reward -16.258 Termination running Distance 10.00 Heading 90.0 deg\n",
      "Episode 1 Step 148 Reward -16.173 Termination running Distance 9.90 Heading 90.0 deg\n",
      "Episode 1 Step 149 Reward -16.064 Termination running Distance 9.78 Heading 90.0 deg\n",
      "Episode 1 Step 150 Reward -15.916 Termination running Distance 9.64 Heading 90.0 deg\n",
      "Episode 1 Step 151 Reward -15.748 Termination running Distance 9.49 Heading 90.0 deg\n",
      "Episode 1 Step 152 Reward -15.556 Termination running Distance 9.32 Heading 90.0 deg\n",
      "Episode 1 Step 153 Reward -15.311 Termination running Distance 9.15 Heading 90.0 deg\n",
      "Episode 1 Step 154 Reward -15.044 Termination running Distance 8.98 Heading 90.0 deg\n",
      "Episode 1 Step 155 Reward -14.794 Termination running Distance 8.81 Heading 90.0 deg\n",
      "Episode 1 Step 156 Reward -14.536 Termination running Distance 8.64 Heading 90.0 deg\n",
      "Episode 1 Step 157 Reward -14.209 Termination running Distance 8.51 Heading 90.0 deg\n",
      "Episode 1 Step 158 Reward -13.954 Termination running Distance 8.41 Heading 90.0 deg\n",
      "Episode 1 Step 159 Reward -13.760 Termination running Distance 8.34 Heading 90.0 deg\n",
      "Episode 1 Step 160 Reward -13.626 Termination running Distance 8.28 Heading 90.0 deg\n",
      "Episode 1 Step 161 Reward -13.510 Termination running Distance 8.24 Heading 90.0 deg\n",
      "Episode 1 Step 162 Reward -13.423 Termination running Distance 8.21 Heading 90.0 deg\n",
      "Episode 1 Step 163 Reward -13.350 Termination running Distance 8.18 Heading 90.0 deg\n",
      "Episode 1 Step 164 Reward -13.282 Termination running Distance 8.17 Heading 90.0 deg\n",
      "Episode 1 Step 165 Reward -13.258 Termination running Distance 8.18 Heading 90.0 deg\n",
      "Episode 1 Step 166 Reward -13.323 Termination running Distance 8.19 Heading 90.0 deg\n",
      "Episode 1 Step 167 Reward -13.405 Termination running Distance 8.22 Heading 90.0 deg\n",
      "Episode 1 Step 168 Reward -13.503 Termination running Distance 8.26 Heading 89.9 deg\n",
      "Episode 1 Step 169 Reward -13.616 Termination running Distance 8.31 Heading 89.9 deg\n",
      "Episode 1 Step 170 Reward -13.744 Termination running Distance 8.37 Heading 89.7 deg\n",
      "Episode 1 Step 171 Reward -13.901 Termination running Distance 8.44 Heading 89.5 deg\n",
      "Episode 1 Step 172 Reward -14.087 Termination running Distance 8.53 Heading 89.3 deg\n",
      "Episode 1 Step 173 Reward -14.289 Termination running Distance 8.63 Heading 88.9 deg\n",
      "Episode 1 Step 174 Reward -14.526 Termination running Distance 8.75 Heading 88.4 deg\n",
      "Episode 1 Step 175 Reward -14.782 Termination running Distance 8.89 Heading 87.8 deg\n",
      "Episode 1 Step 176 Reward -15.071 Termination running Distance 9.05 Heading 87.3 deg\n",
      "Episode 1 Step 177 Reward -15.120 Termination running Distance 9.17 Heading 87.1 deg\n",
      "Episode 1 Step 178 Reward -15.154 Termination running Distance 9.26 Heading 87.0 deg\n",
      "Episode 1 Step 179 Reward -15.166 Termination running Distance 9.32 Heading 87.0 deg\n",
      "Episode 1 Step 180 Reward -15.182 Termination running Distance 9.37 Heading 87.0 deg\n",
      "Episode 1 Step 181 Reward -15.195 Termination running Distance 9.41 Heading 87.0 deg\n",
      "Episode 1 Step 182 Reward -15.204 Termination running Distance 9.44 Heading 87.0 deg\n",
      "Episode 1 Step 183 Reward -15.219 Termination running Distance 9.46 Heading 87.0 deg\n",
      "Episode 1 Step 184 Reward -15.234 Termination running Distance 9.47 Heading 87.0 deg\n",
      "Episode 1 Step 185 Reward -15.245 Termination running Distance 9.49 Heading 87.0 deg\n",
      "Episode 1 Step 186 Reward -15.254 Termination running Distance 9.50 Heading 87.0 deg\n",
      "Episode 1 Step 187 Reward -15.262 Termination running Distance 9.51 Heading 87.0 deg\n",
      "Episode 1 Step 188 Reward -15.259 Termination running Distance 9.52 Heading 87.0 deg\n",
      "Episode 1 Step 189 Reward -15.266 Termination running Distance 9.52 Heading 87.0 deg\n",
      "Episode 1 Step 190 Reward -15.274 Termination running Distance 9.53 Heading 87.0 deg\n",
      "Episode 1 Step 191 Reward -15.276 Termination running Distance 9.53 Heading 87.0 deg\n",
      "Episode 1 Step 192 Reward -15.285 Termination running Distance 9.52 Heading 87.0 deg\n",
      "Episode 1 Step 193 Reward -15.299 Termination running Distance 9.49 Heading 87.0 deg\n",
      "Episode 1 Step 194 Reward -15.241 Termination running Distance 9.47 Heading 87.0 deg\n",
      "Episode 1 Step 195 Reward -15.234 Termination running Distance 9.42 Heading 87.0 deg\n",
      "Episode 1 Step 196 Reward -15.197 Termination running Distance 9.36 Heading 87.0 deg\n",
      "Episode 1 Step 197 Reward -15.146 Termination running Distance 9.29 Heading 87.0 deg\n",
      "Episode 1 Step 198 Reward -15.066 Termination running Distance 9.19 Heading 87.0 deg\n",
      "Episode 1 Step 199 Reward -14.958 Termination running Distance 9.08 Heading 87.0 deg\n",
      "Episode 1 Step 200 Reward -14.747 Termination running Distance 9.00 Heading 87.0 deg\n",
      "Episode 1 Step 201 Reward -14.577 Termination running Distance 8.94 Heading 87.0 deg\n",
      "Episode 1 Step 202 Reward -14.455 Termination running Distance 8.89 Heading 87.0 deg\n",
      "Episode 1 Step 203 Reward -14.366 Termination running Distance 8.86 Heading 87.0 deg\n",
      "Episode 1 Step 204 Reward -14.289 Termination running Distance 8.83 Heading 87.0 deg\n",
      "Episode 1 Step 205 Reward -14.245 Termination running Distance 8.81 Heading 87.0 deg\n",
      "Episode 1 Step 206 Reward -14.249 Termination running Distance 8.78 Heading 87.0 deg\n",
      "Episode 1 Step 207 Reward -14.232 Termination running Distance 8.72 Heading 87.0 deg\n",
      "Episode 1 Step 208 Reward -14.124 Termination running Distance 8.68 Heading 87.0 deg\n",
      "Episode 1 Step 209 Reward -14.037 Termination running Distance 8.65 Heading 87.0 deg\n",
      "Episode 1 Step 210 Reward -14.021 Termination running Distance 8.60 Heading 87.0 deg\n",
      "Episode 1 Step 211 Reward -13.977 Termination running Distance 8.53 Heading 87.0 deg\n",
      "Episode 1 Step 212 Reward -13.853 Termination running Distance 8.49 Heading 87.0 deg\n",
      "Episode 1 Step 213 Reward -13.761 Termination running Distance 8.45 Heading 87.0 deg\n",
      "Episode 1 Step 214 Reward -13.692 Termination running Distance 8.42 Heading 87.0 deg\n",
      "Episode 1 Step 215 Reward -13.684 Termination running Distance 8.38 Heading 87.0 deg\n",
      "Episode 1 Step 216 Reward -13.647 Termination running Distance 8.32 Heading 87.0 deg\n",
      "Episode 1 Step 217 Reward -13.518 Termination running Distance 8.27 Heading 87.0 deg\n",
      "Episode 1 Step 218 Reward -13.433 Termination running Distance 8.24 Heading 87.0 deg\n",
      "Episode 1 Step 219 Reward -13.374 Termination running Distance 8.21 Heading 87.0 deg\n",
      "Episode 1 Step 220 Reward -13.325 Termination running Distance 8.19 Heading 87.0 deg\n",
      "Episode 1 Step 221 Reward -13.267 Termination running Distance 8.19 Heading 87.0 deg\n",
      "Episode 1 Step 222 Reward -13.262 Termination running Distance 8.19 Heading 87.0 deg\n",
      "Episode 1 Step 223 Reward -13.332 Termination running Distance 8.21 Heading 87.0 deg\n",
      "Episode 1 Step 224 Reward -13.434 Termination running Distance 8.24 Heading 87.0 deg\n",
      "Episode 1 Step 225 Reward -13.534 Termination running Distance 8.28 Heading 87.0 deg\n",
      "Episode 1 Step 226 Reward -13.661 Termination running Distance 8.34 Heading 87.0 deg\n",
      "Episode 1 Step 227 Reward -13.815 Termination running Distance 8.41 Heading 87.0 deg\n",
      "Episode 1 Step 228 Reward -13.980 Termination running Distance 8.49 Heading 87.0 deg\n",
      "Episode 1 Step 229 Reward -14.159 Termination running Distance 8.59 Heading 87.0 deg\n",
      "Episode 1 Step 230 Reward -14.368 Termination running Distance 8.69 Heading 87.0 deg\n",
      "Episode 1 Step 231 Reward -14.595 Termination running Distance 8.81 Heading 87.0 deg\n",
      "Episode 1 Step 232 Reward -14.842 Termination running Distance 8.95 Heading 87.0 deg\n",
      "Episode 1 Step 233 Reward -15.119 Termination running Distance 9.10 Heading 87.0 deg\n",
      "Episode 1 Step 234 Reward -15.410 Termination running Distance 9.26 Heading 87.0 deg\n",
      "Episode 1 Step 235 Reward -15.727 Termination running Distance 9.44 Heading 87.0 deg\n",
      "Episode 1 Step 236 Reward -16.070 Termination running Distance 9.63 Heading 87.0 deg\n",
      "Episode 1 Step 237 Reward -16.429 Termination running Distance 9.85 Heading 87.0 deg\n",
      "Episode 1 Step 238 Reward -16.812 Termination running Distance 10.07 Heading 87.0 deg\n",
      "Episode 1 Step 239 Reward -16.888 Termination running Distance 10.24 Heading 87.0 deg\n",
      "Episode 1 Step 240 Reward -16.949 Termination running Distance 10.37 Heading 87.0 deg\n",
      "Episode 1 Step 241 Reward -16.990 Termination running Distance 10.47 Heading 87.0 deg\n",
      "Episode 1 Step 242 Reward -17.024 Termination running Distance 10.55 Heading 87.0 deg\n",
      "Episode 1 Step 243 Reward -17.050 Termination running Distance 10.60 Heading 87.0 deg\n",
      "Episode 1 Step 244 Reward -17.068 Termination running Distance 10.65 Heading 87.0 deg\n",
      "Episode 1 Step 245 Reward -17.074 Termination running Distance 10.68 Heading 87.0 deg\n",
      "Episode 1 Step 246 Reward -17.081 Termination running Distance 10.70 Heading 87.0 deg\n",
      "Episode 1 Step 247 Reward -17.032 Termination running Distance 10.70 Heading 87.0 deg\n",
      "Episode 1 Step 248 Reward -17.044 Termination running Distance 10.69 Heading 87.0 deg\n",
      "Episode 1 Step 249 Reward -17.050 Termination running Distance 10.66 Heading 87.0 deg\n",
      "Episode 1 Step 250 Reward -17.042 Termination running Distance 10.61 Heading 87.0 deg\n",
      "Episode 1 Step 251 Reward -17.000 Termination running Distance 10.54 Heading 87.0 deg\n",
      "Episode 1 Step 252 Reward -16.931 Termination running Distance 10.45 Heading 87.0 deg\n",
      "Episode 1 Step 253 Reward -16.811 Termination running Distance 10.34 Heading 87.0 deg\n",
      "Episode 1 Step 254 Reward -16.694 Termination running Distance 10.22 Heading 87.0 deg\n",
      "Episode 1 Step 255 Reward -16.529 Termination running Distance 10.07 Heading 87.0 deg\n",
      "Episode 1 Step 256 Reward -16.361 Termination running Distance 9.92 Heading 87.0 deg\n",
      "Episode 1 Step 257 Reward -16.154 Termination running Distance 9.74 Heading 87.0 deg\n",
      "Episode 1 Step 258 Reward -15.895 Termination running Distance 9.56 Heading 87.0 deg\n",
      "Episode 1 Step 259 Reward -15.622 Termination running Distance 9.38 Heading 87.0 deg\n",
      "Episode 1 Step 260 Reward -15.356 Termination running Distance 9.20 Heading 87.0 deg\n",
      "Episode 1 Step 261 Reward -15.113 Termination running Distance 9.03 Heading 86.9 deg\n",
      "Episode 1 Step 262 Reward -14.841 Termination running Distance 8.85 Heading 86.9 deg\n",
      "Episode 1 Step 263 Reward -14.595 Termination running Distance 8.68 Heading 86.9 deg\n",
      "Episode 1 Step 264 Reward -14.313 Termination running Distance 8.51 Heading 86.9 deg\n",
      "Episode 1 Step 265 Reward -14.066 Termination running Distance 8.33 Heading 86.9 deg\n",
      "Episode 1 Step 266 Reward -13.717 Termination running Distance 8.20 Heading 86.9 deg\n",
      "Episode 1 Step 267 Reward -13.461 Termination running Distance 8.11 Heading 86.8 deg\n",
      "Episode 1 Step 268 Reward -13.279 Termination running Distance 8.03 Heading 86.8 deg\n",
      "Episode 1 Step 269 Reward -13.125 Termination running Distance 7.97 Heading 86.7 deg\n",
      "Episode 1 Step 270 Reward -13.011 Termination running Distance 7.93 Heading 86.7 deg\n",
      "Episode 1 Step 271 Reward -12.931 Termination running Distance 7.90 Heading 86.6 deg\n",
      "Episode 1 Step 272 Reward -12.856 Termination running Distance 7.87 Heading 86.6 deg\n",
      "Episode 1 Step 273 Reward -12.806 Termination running Distance 7.85 Heading 86.5 deg\n",
      "Episode 1 Step 274 Reward -12.768 Termination running Distance 7.84 Heading 86.5 deg\n",
      "Episode 1 Step 275 Reward -12.745 Termination running Distance 7.83 Heading 86.4 deg\n",
      "Episode 1 Step 276 Reward -12.763 Termination running Distance 7.80 Heading 86.3 deg\n",
      "Episode 1 Step 277 Reward -12.751 Termination running Distance 7.76 Heading 86.0 deg\n",
      "Episode 1 Step 278 Reward -12.716 Termination running Distance 7.70 Heading 85.7 deg\n",
      "Episode 1 Step 279 Reward -12.672 Termination running Distance 7.63 Heading 85.1 deg\n",
      "Episode 1 Step 280 Reward -12.594 Termination running Distance 7.55 Heading 84.4 deg\n",
      "Episode 1 Step 281 Reward -12.501 Termination running Distance 7.45 Heading 83.5 deg\n",
      "Episode 1 Step 282 Reward -12.276 Termination running Distance 7.38 Heading 82.7 deg\n",
      "Episode 1 Step 283 Reward -12.129 Termination running Distance 7.33 Heading 82.0 deg\n",
      "Episode 1 Step 284 Reward -12.019 Termination running Distance 7.29 Heading 81.5 deg\n",
      "Episode 1 Step 285 Reward -11.935 Termination running Distance 7.26 Heading 81.0 deg\n",
      "Episode 1 Step 286 Reward -11.862 Termination running Distance 7.24 Heading 80.6 deg\n",
      "Episode 1 Step 287 Reward -11.878 Termination running Distance 7.21 Heading 80.0 deg\n",
      "Episode 1 Step 288 Reward -11.866 Termination running Distance 7.16 Heading 78.9 deg\n",
      "Episode 1 Step 289 Reward -11.828 Termination running Distance 7.10 Heading 77.3 deg\n",
      "Episode 1 Step 290 Reward -11.756 Termination running Distance 7.03 Heading 75.2 deg\n",
      "Episode 1 Step 291 Reward -11.672 Termination running Distance 6.94 Heading 72.4 deg\n",
      "Episode 1 Step 292 Reward -11.572 Termination running Distance 6.85 Heading 68.9 deg\n",
      "Episode 1 Step 293 Reward -11.320 Termination running Distance 6.77 Heading 65.9 deg\n",
      "Episode 1 Step 294 Reward -11.114 Termination running Distance 6.71 Heading 63.8 deg\n",
      "Episode 1 Step 295 Reward -10.947 Termination running Distance 6.65 Heading 62.4 deg\n",
      "Episode 1 Step 296 Reward -10.891 Termination running Distance 6.58 Heading 60.8 deg\n",
      "Episode 1 Step 297 Reward -10.796 Termination running Distance 6.50 Heading 59.1 deg\n",
      "Episode 1 Step 298 Reward -10.680 Termination running Distance 6.39 Heading 57.3 deg\n",
      "Episode 1 Step 299 Reward -10.543 Termination running Distance 6.26 Heading 55.4 deg\n",
      "Episode 1 Step 300 Reward -10.370 Termination running Distance 6.11 Heading 53.5 deg\n",
      "Episode 1 Step 301 Reward -10.148 Termination running Distance 5.94 Heading 51.7 deg\n",
      "Episode 1 Step 302 Reward -9.875 Termination running Distance 5.75 Heading 50.1 deg\n",
      "Episode 1 Step 303 Reward -9.589 Termination running Distance 5.56 Heading 48.7 deg\n",
      "Episode 1 Step 304 Reward -9.283 Termination running Distance 5.37 Heading 47.6 deg\n",
      "Episode 1 Step 305 Reward -8.989 Termination running Distance 5.18 Heading 46.8 deg\n",
      "Episode 1 Step 306 Reward -8.696 Termination running Distance 4.98 Heading 46.2 deg\n",
      "Episode 1 Step 307 Reward -8.396 Termination running Distance 4.79 Heading 45.8 deg\n",
      "Episode 1 Step 308 Reward -8.100 Termination running Distance 4.59 Heading 45.7 deg\n",
      "Episode 1 Step 309 Reward -7.800 Termination running Distance 4.39 Heading 45.6 deg\n",
      "Episode 1 Step 310 Reward -7.495 Termination running Distance 4.20 Heading 45.7 deg\n",
      "Episode 1 Step 311 Reward -7.207 Termination running Distance 4.00 Heading 45.9 deg\n",
      "Episode 1 Step 312 Reward -6.909 Termination running Distance 3.80 Heading 46.1 deg\n",
      "Episode 1 Step 313 Reward -6.541 Termination running Distance 3.65 Heading 46.4 deg\n",
      "Episode 1 Step 314 Reward -6.271 Termination running Distance 3.54 Heading 46.6 deg\n",
      "Episode 1 Step 315 Reward -6.058 Termination running Distance 3.45 Heading 46.8 deg\n",
      "Episode 1 Step 316 Reward -5.897 Termination running Distance 3.39 Heading 47.0 deg\n",
      "Episode 1 Step 317 Reward -5.764 Termination running Distance 3.33 Heading 47.1 deg\n",
      "Episode 1 Step 318 Reward -5.664 Termination running Distance 3.30 Heading 47.2 deg\n",
      "Episode 1 Step 319 Reward -5.600 Termination running Distance 3.27 Heading 47.3 deg\n",
      "Episode 1 Step 320 Reward -5.548 Termination running Distance 3.25 Heading 47.4 deg\n",
      "Episode 1 Step 321 Reward -5.524 Termination running Distance 3.24 Heading 47.4 deg\n",
      "Episode 1 Step 322 Reward -5.490 Termination running Distance 3.22 Heading 47.5 deg\n",
      "Episode 1 Step 323 Reward -5.493 Termination running Distance 3.19 Heading 47.6 deg\n",
      "Episode 1 Step 324 Reward -5.480 Termination running Distance 3.14 Heading 47.7 deg\n",
      "Episode 1 Step 325 Reward -5.438 Termination running Distance 3.07 Heading 48.0 deg\n",
      "Episode 1 Step 326 Reward -5.313 Termination running Distance 3.02 Heading 48.1 deg\n",
      "Episode 1 Step 327 Reward -5.227 Termination running Distance 2.98 Heading 48.2 deg\n",
      "Episode 1 Step 328 Reward -5.152 Termination running Distance 2.95 Heading 48.2 deg\n",
      "Episode 1 Step 329 Reward -5.096 Termination running Distance 2.93 Heading 48.2 deg\n",
      "Episode 1 Step 330 Reward -5.052 Termination running Distance 2.91 Heading 48.2 deg\n",
      "Episode 1 Step 331 Reward -5.019 Termination running Distance 2.89 Heading 48.2 deg\n",
      "Episode 1 Step 332 Reward -4.991 Termination running Distance 2.88 Heading 48.2 deg\n",
      "Episode 1 Step 333 Reward -4.966 Termination running Distance 2.87 Heading 48.2 deg\n",
      "Episode 1 Step 334 Reward -4.934 Termination running Distance 2.86 Heading 48.1 deg\n",
      "Episode 1 Step 335 Reward -4.923 Termination running Distance 2.86 Heading 48.1 deg\n",
      "Episode 1 Step 336 Reward -4.915 Termination running Distance 2.85 Heading 48.1 deg\n",
      "Episode 1 Step 337 Reward -4.912 Termination running Distance 2.85 Heading 48.1 deg\n",
      "Episode 1 Step 338 Reward -4.912 Termination running Distance 2.85 Heading 48.1 deg\n",
      "Episode 1 Step 339 Reward -4.916 Termination running Distance 2.85 Heading 48.1 deg\n",
      "Episode 1 Step 340 Reward -4.908 Termination running Distance 2.85 Heading 48.0 deg\n",
      "Episode 1 Step 341 Reward -4.937 Termination running Distance 2.83 Heading 47.8 deg\n",
      "Episode 1 Step 342 Reward -4.931 Termination running Distance 2.79 Heading 47.3 deg\n",
      "Episode 1 Step 343 Reward -4.916 Termination running Distance 2.74 Heading 46.6 deg\n",
      "Episode 1 Step 344 Reward -4.861 Termination running Distance 2.67 Heading 45.4 deg\n",
      "Episode 1 Step 345 Reward -4.785 Termination running Distance 2.59 Heading 43.8 deg\n",
      "Episode 1 Step 346 Reward -4.690 Termination running Distance 2.50 Heading 41.7 deg\n",
      "Episode 1 Step 347 Reward -4.489 Termination running Distance 2.43 Heading 39.9 deg\n",
      "Episode 1 Step 348 Reward -4.322 Termination running Distance 2.37 Heading 38.3 deg\n",
      "Episode 1 Step 349 Reward -4.203 Termination running Distance 2.34 Heading 37.0 deg\n",
      "Episode 1 Step 350 Reward -4.110 Termination running Distance 2.31 Heading 35.9 deg\n",
      "Episode 1 Step 351 Reward -4.047 Termination running Distance 2.29 Heading 35.0 deg\n",
      "Episode 1 Step 352 Reward -3.998 Termination running Distance 2.27 Heading 34.2 deg\n",
      "Episode 1 Step 353 Reward -3.991 Termination running Distance 2.25 Heading 32.5 deg\n",
      "Episode 1 Step 354 Reward -3.983 Termination running Distance 2.21 Heading 30.1 deg\n",
      "Episode 1 Step 355 Reward -3.946 Termination running Distance 2.16 Heading 26.8 deg\n",
      "Episode 1 Step 356 Reward -3.877 Termination running Distance 2.10 Heading 22.6 deg\n",
      "Episode 1 Step 357 Reward -3.783 Termination running Distance 2.02 Heading 17.6 deg\n",
      "Episode 1 Step 358 Reward -3.562 Termination running Distance 1.96 Heading 13.8 deg\n",
      "Episode 1 Step 359 Reward -3.389 Termination running Distance 1.91 Heading 10.8 deg\n",
      "Episode 1 Step 360 Reward -3.248 Termination running Distance 1.86 Heading 8.9 deg\n",
      "Episode 1 Step 361 Reward -3.139 Termination running Distance 1.83 Heading 7.6 deg\n",
      "Episode 1 Step 362 Reward -3.065 Termination running Distance 1.81 Heading 6.8 deg\n",
      "Episode 1 Step 363 Reward -3.008 Termination running Distance 1.79 Heading 6.2 deg\n",
      "Episode 1 Step 364 Reward -2.969 Termination running Distance 1.78 Heading 5.9 deg\n",
      "Episode 1 Step 365 Reward -2.940 Termination running Distance 1.77 Heading 5.7 deg\n",
      "Episode 1 Step 366 Reward -2.937 Termination running Distance 1.77 Heading 5.6 deg\n",
      "Episode 1 Step 367 Reward -2.917 Termination running Distance 1.76 Heading 5.5 deg\n",
      "Episode 1 Step 368 Reward -2.905 Termination running Distance 1.75 Heading 5.4 deg\n",
      "Episode 1 Step 369 Reward -2.922 Termination running Distance 1.73 Heading 5.0 deg\n",
      "Episode 1 Step 370 Reward -2.916 Termination running Distance 1.69 Heading 4.5 deg\n",
      "Episode 1 Step 371 Reward -2.873 Termination running Distance 1.62 Heading 3.7 deg\n",
      "Episode 1 Step 372 Reward -2.798 Termination running Distance 1.54 Heading 2.8 deg\n",
      "Episode 1 Step 373 Reward -2.698 Termination running Distance 1.44 Heading 1.8 deg\n",
      "Episode 1 Step 374 Reward -2.504 Termination running Distance 1.36 Heading 1.1 deg\n",
      "Episode 1 Step 375 Reward -2.354 Termination running Distance 1.31 Heading 0.7 deg\n",
      "Episode 1 Step 376 Reward -2.250 Termination running Distance 1.26 Heading 0.5 deg\n",
      "Episode 1 Step 377 Reward -2.172 Termination running Distance 1.23 Heading 0.3 deg\n",
      "Episode 1 Step 378 Reward -2.094 Termination running Distance 1.20 Heading 0.2 deg\n",
      "Episode 1 Step 379 Reward -2.043 Termination running Distance 1.18 Heading 0.2 deg\n",
      "Episode 1 Step 380 Reward -1.994 Termination running Distance 1.16 Heading 0.2 deg\n",
      "Episode 1 Step 381 Reward -1.956 Termination running Distance 1.15 Heading 0.2 deg\n",
      "Episode 1 Step 382 Reward -1.972 Termination running Distance 1.12 Heading 0.2 deg\n",
      "Episode 1 Step 383 Reward -1.963 Termination running Distance 1.07 Heading 0.2 deg\n",
      "Episode 1 Step 384 Reward -1.917 Termination running Distance 1.00 Heading 0.2 deg\n",
      "Episode 1 Step 385 Reward -1.839 Termination running Distance 0.91 Heading 0.2 deg\n",
      "Episode 1 Step 386 Reward -1.733 Termination running Distance 0.80 Heading 0.2 deg\n",
      "Episode 1 Step 387 Reward -1.533 Termination running Distance 0.72 Heading 0.2 deg\n",
      "Episode 1 Step 388 Reward -1.433 Termination running Distance 0.62 Heading 0.2 deg\n",
      "Episode 1 Step 389 Reward -1.313 Termination running Distance 0.50 Heading 0.2 deg\n",
      "Episode 1 Step 390 Reward -1.167 Termination running Distance 0.36 Heading 0.2 deg\n",
      "Episode 1 Step 391 Reward -0.991 Termination running Distance 0.20 Heading 0.2 deg\n",
      "Episode 1 Step 392 Reward -0.680 Termination running Distance 0.08 Heading 0.2 deg\n",
      "Episode 1 Step 393 Reward -0.508 Termination running Distance 0.02 Heading 0.2 deg\n",
      "Episode 1 Step 394 Reward -0.535 Termination running Distance 0.09 Heading 0.2 deg\n",
      "Episode 1 Step 395 Reward -0.559 Termination running Distance 0.14 Heading 0.2 deg\n",
      "Episode 1 Step 396 Reward -0.577 Termination running Distance 0.17 Heading 0.2 deg\n",
      "Episode 1 Step 397 Reward 139.412 Termination success Distance 0.20 Heading 0.2 deg\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['conda', 'run', '-n', 'parking-rl', 'python', 'main.py', '--mode', 'manual', '--episodes', '1', '--max-steps', '4000', '--config', '/home/ansatz/ME5418/parking_project/generated_configs/notebook_override.json'], returncode=0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "project_root = Path(\".\").resolve()\n",
    "cmd = [\n",
    "    \"conda\", \"run\", \"-n\", \"parking-rl\", \"python\", \"main.py\",\n",
    "    \"--mode\", mode,\n",
    "    \"--episodes\", str(episodes),\n",
    "    \"--max-steps\", str(max_steps),\n",
    "]\n",
    "\n",
    "if sleep_scale != 0.5:\n",
    "    cmd.extend([\"--sleep-scale\", str(sleep_scale)])\n",
    "\n",
    "if config_path is not None:\n",
    "    if write_notebook_config:\n",
    "        config_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with config_path.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "            json.dump(custom_config, fh, indent=2, ensure_ascii=False)\n",
    "            fh.write(\"\\n\")\n",
    "    cmd.extend([\"--config\", str(config_path.resolve())])\n",
    "\n",
    "print(\"Launching CLI demo:\", \" \".join(cmd))\n",
    "env_vars = os.environ.copy()\n",
    "env_vars[\"MPLBACKEND\"] = \"QtAgg\"\n",
    "subprocess.run(cmd, cwd=project_root, env=env_vars, check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dde97c",
   "metadata": {},
   "source": [
    "## 训练接入说明（中文说明）\n",
    "\n",
    "- 在训练脚本中可以 `env = gym.make(\"ParkingEnv-v0\", config=custom_cfg)`，或从 `parking_gym import ParkingEnv` 自行实例化；省略 `config` 时会采用默认参数（包含 9 束雷达）。\n",
    "- 观测向量长度为 `11 + len(ray_angles)`，可通过 `env.observation_space.shape[0]` 或一次 `obs, _ = env.reset()` 验证；顺序与上文状态列表一致，末尾为 0~1 归一化的激光距离。\n",
    "- `env.step(action)` 需要形如 `np.array([lon_accel, steer_accel], dtype=np.float32)` 的二维动作，可参考下方示例。\n",
    "- 若需关闭观测噪声以便评估，可调用 `env.unwrapped.set_observation_noise(enabled=False)`；重新开启或调整标准差时通过 `std` 参数指定。\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from parking_gym import ParkingEnv\n",
    "\n",
    "env = ParkingEnv()  # 或 gym.make(\"ParkingEnv-v0\")\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.array([0.0, 0.0], dtype=np.float32)  # 替换为策略输出\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e99629",
   "metadata": {},
   "source": [
    "## Training Integration Notes (English Explanation)\n",
    "\n",
    "- In your training script call `env = gym.make(\"ParkingEnv-v0\", config=custom_cfg)` or import `ParkingEnv` directly; omitting `config` uses the synced defaults (with nine lidar beams).\n",
    "- The observation vector length is `11 + len(ray_angles)`; confirm with `env.observation_space.shape[0]` or by running `obs, _ = env.reset()`. The order matches the earlier state list, with the lidar distances occupying the tail.\n",
    "- `env.step(action)` expects a two-dimensional action such as `np.array([lon_accel, steer_accel], dtype=np.float32)`; see the snippet below.\n",
    "- Disable observation noise during evaluation via `env.unwrapped.set_observation_noise(enabled=False)`, or adjust the standard deviation by passing a `std` value.\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from parking_gym import ParkingEnv\n",
    "\n",
    "env = ParkingEnv()  # or gym.make(\"ParkingEnv-v0\")\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.array([0.0, 0.0], dtype=np.float32)  # replace with policy output\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parking-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
