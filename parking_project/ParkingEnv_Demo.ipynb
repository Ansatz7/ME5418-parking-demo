{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ParkingEnv 交互式演示（中文说明）\n",
    "\n",
    "这个笔记本帮助你在 Jupyter 中配置并启动 `main.py` 或助力调参器，快速体验停车环境。\n",
    "\n",
    "- 先设置下一个代码单元中的运行参数（轮次数、最大步数、模式、动画速度）。\n",
    "- 修改参数后重新运行相关单元即可让 CLI 使用新的值。\n",
    "- `manual` 模式下，Qt 窗口使用方向键：`↑/↓` 控制油门，`←/→` 控制方向，`Esc` 退出。\n",
    "- 每个代码单元都会说明是否写入 JSON 配置，按需调整文件路径后执行即可。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4785e20e",
   "metadata": {},
   "source": [
    "# ParkingEnv Interactive Demo (English Notes)\n",
    "\n",
    "This notebook shows how to configure and launch `main.py` or the assist-model tuner straight from Jupyter.\n",
    "\n",
    "- Set the run parameters in the next code cell (episode count, max steps, mode, animation speed).\n",
    "- Rerun the relevant cells after changing a value so the CLI picks up the new settings.\n",
    "- In `manual` mode the Qt window listens to the arrow keys: `↑/↓` throttle, `←/→` steering, `Esc` to exit.\n",
    "- Each code cell tells you whether it writes a JSON config; adjust the file paths first if needed, then execute it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca995f54",
   "metadata": {},
   "source": [
    "## 运行参数设置（中文说明）\n",
    "\n",
    "下方代码块定义 CLI 演示所需的基础变量。\n",
    "\n",
    "- `episodes` 控制重复运行次数，通常 1 次即可快速检查界面。\n",
    "- `max_steps` 设定每轮的最大仿真步数，避免策略长时间卡住。\n",
    "- `sleep_scale` 调整动画节奏，值越大播放越慢，便于观察。\n",
    "- `mode` 选择 `manual` 或 `random`，也可以在下方保留备用模式以便切换。\n",
    "- 如需更多自定义项，可在运行后直接修改变量并重新执行。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70adc0c5",
   "metadata": {},
   "source": [
    "## Run Parameter Setup (English Notes)\n",
    "\n",
    "The next code cell declares the baseline variables used by the CLI demo.\n",
    "\n",
    "- `episodes` controls how many times the scenario repeats; a single run is usually enough to verify the UI.\n",
    "- `max_steps` caps the number of simulation steps per episode so random policies cannot run forever.\n",
    "- `sleep_scale` slows down the animation when set above `1.0`, which helps when you want to inspect behaviour frame by frame.\n",
    "- `mode` toggles between `manual` and `random`; keep an alternate assignment commented out for quick switching.\n",
    "- Feel free to add more parameters or tweak these values, then rerun the cell to update the downstream commands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "81db3228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "episodes = 1\n",
    "max_steps = 4000\n",
    "sleep_scale = 0.5    # Larger values slow the animation\n",
    "\n",
    "mode = \"manual\"  # \"manual\" or \"random\"\n",
    "# mode = \"random\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f42a67",
   "metadata": {},
   "source": [
    "## 随机生成训练配置（中文说明）\n",
    "\n",
    "下方命令会调用 `generate_training_config.py`，只随机化车位和障碍布局，并将结果写入 `generated_configs/train_001.json`。\n",
    "\n",
    "- 执行前请先在外部终端激活 `parking-rl` Conda 环境，并确保依赖已经安装。\n",
    "- 如果不想覆盖现有文件，可修改 `--out` 路径或暂时注释这一行。\n",
    "- 想直接复用旧配置时可以跳过此单元格。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70601dfb",
   "metadata": {},
   "source": [
    "## Random Training Config Generator (English Notes)\n",
    "\n",
    "The next command runs `generate_training_config.py`, which randomizes only the parking-slot map parameters and writes them to `generated_configs/train_001.json`.\n",
    "\n",
    "- Activate the `parking-rl` Conda environment in an external terminal beforehand and ensure the dependencies are installed.\n",
    "- Edit or comment out the `--out` option if you prefer not to overwrite an existing file.\n",
    "- Skip this cell whenever you want to reuse a configuration that is already on disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "5332545c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote config to generated_configs/train_001.json\n"
     ]
    }
   ],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# !conda activate parking-rl\n",
    "!python generate_training_config.py --out generated_configs/train_001.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a98c73",
   "metadata": {},
   "source": [
    "## 指定配置文件路径（中文说明）\n",
    "\n",
    "这个单元负责控制是否写入 Notebook 生成的覆盖文件，以及 CLI 将加载哪份 JSON。\n",
    "\n",
    "- 将 `write_notebook_config` 设为 `True` 时，会把当前 `custom_config` 写回磁盘；默认 `False` 以保护现有文件。\n",
    "- 推荐默认指向 `generated_configs/notebook_override.json`，也可以改为训练配置或其他路径。\n",
    "- 调整 `config_path` 后请重新运行依赖该变量的后续单元。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0cfdf4",
   "metadata": {},
   "source": [
    "## Choose the Config File Path (English Notes)\n",
    "\n",
    "This code controls whether the notebook writes the override JSON and which file the CLI loads.\n",
    "\n",
    "- Set `write_notebook_config` to `True` when you want to persist `custom_config`; leave it `False` to avoid overwriting existing files.\n",
    "- The recommended default is `generated_configs/notebook_override.json`, but you can switch to the training config or any custom path.\n",
    "- After changing `config_path`, rerun the downstream cells that depend on it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "0f04f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_notebook_config = False  # Set to False to keep existing file contents\n",
    "# config_path = Path(\"generated_configs/notebook_override.json\") # use default config for notebook\n",
    "config_path = Path(\"generated_configs/train_001.json\") # use training config for notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe333259",
   "metadata": {},
   "source": [
    "## 载入默认配置副本（中文说明）\n",
    "\n",
    "以下代码会深拷贝 `build_config()` 返回的默认配置，并在目标 JSON 存在时读取它。\n",
    "\n",
    "- 如果 `config_path` 指向的文件存在，会优先加载其中的参数（保留 `__notes` 等附加信息）。\n",
    "- 若文件不存在，就使用默认配置并存入 `custom_config`，方便在 Notebook 中即时修改。\n",
    "- 可以利用末尾的示例查看或编辑任意字段，调整后配合写回逻辑即可保存。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df018bc3",
   "metadata": {},
   "source": [
    "## Load a Working Copy of the Config (English Notes)\n",
    "\n",
    "The next cell deep-copies the defaults returned by `build_config()` and reads the target JSON when it is available.\n",
    "\n",
    "- When `config_path` already exists, its contents are loaded first, including any extra fields such as `__notes`.\n",
    "- If the file is missing, the code falls back to the default dictionary and keeps it in `custom_config` for quick edits.\n",
    "- Use the examples at the end of the cell to inspect or update individual fields, then combine with the write-back logic to persist your changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "1277a326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dt': 0.1,\n",
       " 'max_steps': 4000,\n",
       " 'field_size': 60.0,\n",
       " 'ray_max_range': 12.0,\n",
       " 'observation_noise': {'enabled': True, 'std': 0.005},\n",
       " 'ray_angles': [-135.0, -90.0, -60.0, -30.0, 30.0, 60.0, 90.0, 135.0, 0.0],\n",
       " 'vehicle': {'length': 4.0,\n",
       "  'width': 2.0,\n",
       "  'wheel_base': 2.5,\n",
       "  'max_speed': 3.0,\n",
       "  'max_reverse_speed': -2.0,\n",
       "  'max_steering_angle': 60.0,\n",
       "  'max_steering_rate': 30.0,\n",
       "  'steering_damping': 52.5,\n",
       "  'steering_rate_damping': 8.75,\n",
       "  'steering_assist_deadband': 0.03,\n",
       "  'velocity_damping': 2.45,\n",
       "  'velocity_deadband': 0.03,\n",
       "  'enable_steering_assist': True,\n",
       "  'manual_forward_accel': 1.5,\n",
       "  'manual_reverse_accel': 2.0,\n",
       "  'manual_steering_accel': 10.0},\n",
       " 'spawn_region': [-0.91, 15.03, -0.76, 15.02],\n",
       " 'parking_slot': {'length': 5.9,\n",
       "  'width': 2.35,\n",
       "  'offset_x_range': [-21.4, -14.51],\n",
       "  'offset_y_range': [-18.46, -4.78],\n",
       "  'orientation_range': [-10.85, 10.85]},\n",
       " 'static_obstacles': {'count': 4,\n",
       "  'size_range': [0.95, 2.42],\n",
       "  'min_distance': 1.67,\n",
       "  'seed': 2824},\n",
       " 'dynamic_obstacles': {'count': 0,\n",
       "  'radius': 0.83,\n",
       "  'speed_range': [0.62, 1.21],\n",
       "  'behavior': 'goal_driven',\n",
       "  'min_distance': 3.9,\n",
       "  'heading_noise': 17.65},\n",
       " 'reward': {'distance_scale': 1.5,\n",
       "  'heading_scale': 0.5,\n",
       "  'collision': -120.0,\n",
       "  'success': 140.0,\n",
       "  'smoothness': 0.05,\n",
       "  'step_cost': 0.2,\n",
       "  'velocity_penalty': 0.3},\n",
       " 'success_thresholds': {'position': 0.4,\n",
       "  'orientation': 6.0,\n",
       "  'speed': 0.3,\n",
       "  'steering': 5.0},\n",
       " 'rng_seed': 660826}"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import json\n",
    "from main import build_config\n",
    "\n",
    "# Load existing notebook override (keeps __notes) when available; fall back to defaults otherwise.\n",
    "if config_path.exists():\n",
    "    with config_path.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "        custom_config = json.load(fh)\n",
    "else:\n",
    "    custom_config = deepcopy(build_config())\n",
    "\n",
    "# You can modify custom_config here if desired, e.g.:\n",
    "custom_config # shows all config parameters, including defaults\n",
    "# custom_config[\"vehicle\"][\"length\"] # show specific parameter\n",
    "# custom_config[\"vehicle\"][\"length\"] = 4.5 # modify specific parameter\n",
    "# custom_config[\"vehicle\"][\"length\"] # verify change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b6b1e0",
   "metadata": {},
   "source": [
    "## 状态 / 动作 / 奖励 总览（中文说明）\n",
    "\n",
    "**状态空间**\n",
    "- 全局位置 `(x, y)`：车辆中心在世界坐标系中的位置，内部已按 `±field_size/2` 归一化；实际训练时可再做特征缩放。\n",
    "- `cos(yaw)` 与 `sin(yaw)`：车辆朝向的三角编码，避免角度取模问题，数值范围 [-1, 1]。\n",
    "- 纵向速度 `velocity`：受 `max_speed` 与 `max_reverse_speed` 限制，典型区间约 [-2.0, 3.0] m/s。\n",
    "- 转向角 `steering_angle`：裁剪在 `±max_steering_angle` 内，默认约 ±0.79 rad (±45°)。\n",
    "- 转向角速度 `steering_rate`：裁剪在 `±max_steering_rate` 内，默认约 ±1.05 rad/s (±60°/s)。\n",
    "- 车位坐标误差 `(slot_dx, slot_dy)`：车辆在车位坐标系下的平移偏差，采用场地大小归一化。\n",
    "- `cos(Δyaw)` 与 `sin(Δyaw)`：车身与车位朝向差的三角编码，范围 [-1, 1]。\n",
    "- 激光测距 `ray_1 … ray_N`：共 `N = len(ray_angles)` 束射线，取值 [0, 1]，0 表示立即碰撞，1 表示达到 `ray_max_range`。\n",
    "- 除非使用 `raw=True`，观测默认叠加标准差为 0.005 的高斯噪声，可通过 `ParkingEnv(config={\"observation_noise\": {\"enabled\": False}})` 或 `env.unwrapped.set_observation_noise(...)` 关闭/调整。\n",
    "\n",
    "**动作空间（Box(2))**\n",
    "- `a_longitudinal`：纵向加速度指令，范围 [-3.0, 2.0] m/s²，正值加速、负值制动或倒车。\n",
    "- `a_steering`：转向角加速度指令，范围 [-1.5, 1.5] rad/s²；在 `step()` 中会裁剪到上限以保持稳定。\n",
    "\n",
    "**奖励构成**\n",
    "- `distance`：`-distance_scale * ||slot_xy||`，鼓励尽快贴近车位中心。\n",
    "- `heading`：`-heading_scale * |Δyaw|`，惩罚朝向误差。\n",
    "- `velocity`：`-velocity_penalty * |v|`，在靠近车位时鼓励降速。\n",
    "- `smoothness`：`-smoothness * steering_rate^2`，约束方向盘抖动，与辅助模型阻尼相关。\n",
    "- `step`：每步扣除 `step_cost`，促使策略提高效率。\n",
    "- `collision`：碰撞时一次扣除 `collision`（默认 -120）。\n",
    "- `success`：满足位置、朝向与速度阈值时给予 `success` 奖励（默认 +140）。\n",
    "- 以上权重均来源于 `config['reward']`，可在 JSON 中调节以改变训练侧重点。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010288dd",
   "metadata": {},
   "source": [
    "## State / Action / Reward Overview (English Notes)\n",
    "\n",
    "**State space**\n",
    "- Global position `(x, y)`: vehicle centre in world coordinates, internally normalized by `±field_size/2`; you may apply additional feature scaling for training.\n",
    "- `cos(yaw)` and `sin(yaw)`: sine/cosine encoding of the vehicle heading to avoid wrap-around issues, each within [-1, 1].\n",
    "- Longitudinal velocity `velocity`: bounded by `max_speed` and `max_reverse_speed`, typically around [-2.0, 3.0] m/s.\n",
    "- Steering angle `steering_angle`: clipped to `±max_steering_angle`, roughly ±0.79 rad (±45°) by default.\n",
    "- Steering rate `steering_rate`: clipped to `±max_steering_rate`, roughly ±1.05 rad/s (±60°/s) by default.\n",
    "- Slot-frame error `(slot_dx, slot_dy)`: translation of the vehicle relative to the slot frame, normalized by the field size.\n",
    "- `cos(Δyaw)` and `sin(Δyaw)`: heading difference between the vehicle and the slot, encoded in [-1, 1].\n",
    "- Range readings `ray_1 … ray_N`: `N = len(ray_angles)` lidar beams scaled to [0, 1]; 0 means immediate collision, 1 reaches `ray_max_range`.\n",
    "- Unless you request `raw=True`, a Gaussian noise with std 0.005 is added; disable or tune it via `ParkingEnv(config={\"observation_noise\": {\"enabled\": False}})` or `env.unwrapped.set_observation_noise(...)`.\n",
    "\n",
    "**Action space (Box(2))**\n",
    "- `a_longitudinal`: longitudinal acceleration command in [-3.0, 2.0] m/s²; positive accelerates forward, negative brakes or reverses.\n",
    "- `a_steering`: steering angular acceleration command in [-1.5, 1.5] rad/s²; the environment clips it during `step()` for stability.\n",
    "\n",
    "**Reward components**\n",
    "- `distance`: `-distance_scale * ||slot_xy||`, encouraging the agent to reach the slot centre quickly.\n",
    "- `heading`: `-heading_scale * |Δyaw|`, penalising heading misalignment.\n",
    "- `velocity`: `-velocity_penalty * |v|`, encouraging the car to slow down near the slot.\n",
    "- `smoothness`: `-smoothness * steering_rate^2`, discouraging steering oscillations and linking to the assist-model damping.\n",
    "- `step`: subtracts `step_cost` every timestep to promote efficiency.\n",
    "- `collision`: applies `collision` once on impact (default -120).\n",
    "- `success`: grants `success` when position, heading, and velocity thresholds are met (default +140).\n",
    "- All weights originate from `config['reward']`, so adjust the JSON to shift training priorities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a473cf2",
   "metadata": {},
   "source": [
    "## 助力模型说明（中文说明）\n",
    "\n",
    "- 当方向盘角加速度指令满足 `|α_cmd| ≤ deadband` 时（即松开方向盘），系统触发回正模型。\n",
    "- 回正加速度 `α_assist = K_p · φ + K_d · φ̇`，并在积分阶段与驾驶员指令叠加更新方向盘角速度与角度。\n",
    "- 油门松开时，若纵向加速度满足 `|a_cmd| ≤ velocity_deadband`，则阻尼 `a_assist = K_v · v` 让车辆速度指数衰减。\n",
    "- 下方代码提供调参器的初始角度、角速度、仿真步数以及是否实时写回 JSON 的选项，便于快速尝试不同阻尼参数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401ad49a",
   "metadata": {},
   "source": [
    "## Assist-Model Notes (English Explanation)\n",
    "\n",
    "- When the steering angular acceleration command satisfies `|α_cmd| ≤ deadband` (driver releases the wheel), the assist model recenters it.\n",
    "- The assist acceleration follows `α_assist = K_p · φ + K_d · φ̇` and combines with the command during integration to update steering rate and angle.\n",
    "- When the throttle command magnitude stays within `velocity_deadband`, the damping term `a_assist = K_v · v` makes the velocity decay exponentially.\n",
    "- The next code cell sets the tuner defaults: initial steering angle/rate, simulation steps, and whether live edits sync back into the JSON file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "cd6af483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assist-model tuner launch options\n",
    "tuner_angle0 = 20.0   # 初始方向盘角度（度）\n",
    "tuner_rate0 = 0.0    # 初始方向盘角速度（度/秒）\n",
    "tuner_steps = 100     # 仿真步数\n",
    "tuner_sync_updates = True  # True 表示实时写回 JSON 配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5110eabc",
   "metadata": {},
   "source": [
    "## 启动助力调参器（中文说明）\n",
    "\n",
    "下面的命令会调用 `assist_model_tuner.py`，在 Qt 窗口中展示方向盘与油门阻尼的仿真结果。你可以滑动滑块来调节来调整阻尼参数，关闭窗口后自动保存。\n",
    "\n",
    "- 使用 `conda run -n parking-rl` 以 notebook 外部环境启动脚本，确保 Matplotlib 后端配置为 `QtAgg`。\n",
    "- 当 `tuner_sync_updates = True` 时，会把当前 `custom_config` 写入 JSON 并作为 `--config` 传入，让调参器实时读取。\n",
    "- 需要额外参数（例如时间步长）时，可在列表中继续追加命令行选项。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a07543f",
   "metadata": {},
   "source": [
    "## Launch the Assist Tuner (English Notes)\n",
    "\n",
    "The upcoming command runs `assist_model_tuner.py` and opens a Qt window that visualises the steering/throttle damping response. You can adjust the damping parameter by sliding the slider and save it automatically after closing the window.\n",
    "\n",
    "- It relies on `conda run -n parking-rl` so that the external environment provides the correct dependencies and the Matplotlib backend is set to `QtAgg`.\n",
    "- When `tuner_sync_updates = True`, the notebook writes `custom_config` to JSON and passes it via `--config`, letting the tuner pick up live edits.\n",
    "- Add further command-line options to `tuner_cmd` if you need extra knobs such as timestep or logging verbosity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "00e2aa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching assist model tuner: conda run -n parking-rl python assist_model_tuner.py --angle0 20.0 --rate0 0.0 --steps 100 --sync --config /home/ansatz/ME5418/parking_project/generated_configs/train_001.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['conda', 'run', '-n', 'parking-rl', 'python', 'assist_model_tuner.py', '--angle0', '20.0', '--rate0', '0.0', '--steps', '100', '--sync', '--config', '/home/ansatz/ME5418/parking_project/generated_configs/train_001.json'], returncode=0)"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "project_root = Path(\".\").resolve()\n",
    "tuner_cmd = [\n",
    "    \"conda\", \"run\", \"-n\", \"parking-rl\", \"python\", \"assist_model_tuner.py\",\n",
    "    \"--angle0\", str(tuner_angle0),\n",
    "    \"--rate0\", str(tuner_rate0),\n",
    "    \"--steps\", str(tuner_steps),\n",
    "]\n",
    "\n",
    "if tuner_sync_updates:\n",
    "    tuner_cmd.append(\"--sync\")\n",
    "\n",
    "if config_path is not None:\n",
    "    if write_notebook_config:\n",
    "        config_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with config_path.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "            json.dump(custom_config, fh, indent=2, ensure_ascii=False)\n",
    "            fh.write(\"\\n\")\n",
    "    tuner_cmd.extend([\"--config\", str(config_path.resolve())])\n",
    "\n",
    "print(\"Launching assist model tuner:\", \" \".join(tuner_cmd))\n",
    "env_vars = os.environ.copy()\n",
    "env_vars[\"MPLBACKEND\"] = \"QtAgg\"\n",
    "subprocess.run(tuner_cmd, cwd=project_root, env=env_vars, check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61601c78",
   "metadata": {},
   "source": [
    "## 查看调参结果（中文说明）\n",
    "\n",
    "调参器运行后，可以执行下方代码读取当前配置文件，并打印关键阻尼参数以供核对。\n",
    "\n",
    "- 若 `write_notebook_config` 为 True，则会从刚刚写入的 JSON 读取数据；否则直接读取现有文件。\n",
    "- 输出的字段包括方向盘比例/阻尼、死区以及油门阻尼，便于确认是否符合预期。\n",
    "- 想检查更多字段时，可继续访问 `custom_config` 中的其他键值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c9bebe",
   "metadata": {},
   "source": [
    "## Inspect Tuner Outputs (English Notes)\n",
    "\n",
    "After the tuner window closes, the next cell reloads the config file and prints the key damping parameters for verification.\n",
    "\n",
    "- When `write_notebook_config` is True, it reads the JSON that was just written; otherwise it loads the existing file on disk.\n",
    "- The printed values cover the steering proportional/derivative gains, deadband, and throttle damping so you can confirm the latest edits.\n",
    "- Feel free to probe additional entries inside `custom_config` if you need more diagnostics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "1e12c9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steering_damping Kp: 52.5\n",
      "steering_rate_damping Kd: 8.75\n",
      "steering_deadband: 0.03\n",
      "velocity_damping Kv: 2.45\n",
      "velocity_deadband: 0.03\n"
     ]
    }
   ],
   "source": [
    "# 调参可视化会在外部 Qt 窗口中渲染，运行上一个单元格即可启动。\n",
    "custom_config = json.load(config_path.open(\"r\", encoding=\"utf-8\"))\n",
    "vehicle_section = custom_config.get(\"vehicle\", {})\n",
    "print(\"steering_damping Kp:\", vehicle_section.get(\"steering_damping\"))  # 输出当前转向比例系数\n",
    "print(\"steering_rate_damping Kd:\", vehicle_section.get(\"steering_rate_damping\"))  # 输出当前转向速度阻尼\n",
    "print(\"steering_deadband:\", vehicle_section.get(\"steering_assist_deadband\"))  # 方向盘回正死区\n",
    "print(\"velocity_damping Kv:\", vehicle_section.get(\"velocity_damping\"))  # 油门释放阻尼\n",
    "print(\"velocity_deadband:\", vehicle_section.get(\"velocity_deadband\"))  # 油门释放死区"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e80f2",
   "metadata": {},
   "source": [
    "## 启动 CLI 演示（中文说明）\n",
    "\n",
    "接下来的代码将生成命令并运行 `main.py`，在 Qt 窗口中展示手动或随机策略。\n",
    "\n",
    "- 运行前请确认已经在外部终端激活 `parking-rl` 环境，确保依赖和 GUI 后端可用。\n",
    "- 如果勾选写入选项，会把当前 `custom_config` 保存成 JSON 并随命令加载。\n",
    "- 想修改睡眠倍速、轮次数等参数时，返回上方变量单元重新运行即可生效。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd74df14",
   "metadata": {},
   "source": [
    "## Launch the CLI Demo (English Notes)\n",
    "\n",
    "The next cell assembles the command for `main.py` and opens a Qt window to run either the manual or random policy.\n",
    "\n",
    "- Make sure the `parking-rl` environment is activated in an external terminal so that dependencies and the GUI backend are available.\n",
    "- When the write flag is enabled, the current `custom_config` is saved to JSON and passed to the CLI via `--config`.\n",
    "- To adjust sleep scaling, episode count, or any other parameter, go back to the earlier variable cell, tweak the values, and rerun it before launching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "fb66c6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching CLI demo: conda run -n parking-rl python main.py --mode manual --episodes 1 --max-steps 4000 --config /home/ansatz/ME5418/parking_project/generated_configs/train_001.json\n",
      "Episode 1 Step 1 Reward -52.958 Termination running Distance 35.08 Heading 15.2 deg\n",
      "Episode 1 Step 2 Reward -52.957 Termination running Distance 35.08 Heading 15.2 deg\n",
      "Episode 1 Step 3 Reward -52.959 Termination running Distance 35.08 Heading 15.2 deg\n",
      "Episode 1 Step 4 Reward -52.972 Termination running Distance 35.08 Heading 15.2 deg\n",
      "Episode 1 Step 5 Reward -52.975 Termination running Distance 35.09 Heading 15.2 deg\n",
      "Episode 1 Step 6 Reward -52.975 Termination running Distance 35.09 Heading 15.2 deg\n",
      "Episode 1 Step 7 Reward -52.974 Termination running Distance 35.09 Heading 15.2 deg\n",
      "Episode 1 Step 8 Reward -52.977 Termination running Distance 35.09 Heading 15.2 deg\n",
      "Episode 1 Step 9 Reward -52.977 Termination running Distance 35.09 Heading 15.2 deg\n",
      "Episode 1 Step 10 Reward -52.967 Termination running Distance 35.09 Heading 15.2 deg\n",
      "Episode 1 Step 11 Reward -52.971 Termination running Distance 35.09 Heading 15.2 deg\n",
      "Episode 1 Step 12 Reward -52.978 Termination running Distance 35.09 Heading 15.2 deg\n",
      "Episode 1 Step 13 Reward -52.985 Termination running Distance 35.09 Heading 15.2 deg\n",
      "Episode 1 Step 14 Reward -52.979 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 15 Reward -52.981 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 16 Reward -52.984 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 17 Reward -52.985 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 18 Reward -52.988 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 19 Reward -52.988 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 20 Reward -52.984 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 21 Reward -52.985 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 22 Reward -52.990 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 23 Reward -52.993 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 24 Reward -53.004 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 25 Reward -53.004 Termination running Distance 35.09 Heading 15.2 deg\n",
      "Episode 1 Step 26 Reward -53.012 Termination running Distance 35.06 Heading 15.2 deg\n",
      "Episode 1 Step 27 Reward -52.999 Termination running Distance 35.01 Heading 15.2 deg\n",
      "Episode 1 Step 28 Reward -52.958 Termination running Distance 34.95 Heading 15.2 deg\n",
      "Episode 1 Step 29 Reward -52.898 Termination running Distance 34.86 Heading 15.2 deg\n",
      "Episode 1 Step 30 Reward -52.801 Termination running Distance 34.76 Heading 15.2 deg\n",
      "Episode 1 Step 31 Reward -52.679 Termination running Distance 34.63 Heading 15.2 deg\n",
      "Episode 1 Step 32 Reward -52.519 Termination running Distance 34.49 Heading 15.2 deg\n",
      "Episode 1 Step 33 Reward -52.334 Termination running Distance 34.33 Heading 15.2 deg\n",
      "Episode 1 Step 34 Reward -52.119 Termination running Distance 34.15 Heading 15.2 deg\n",
      "Episode 1 Step 35 Reward -51.885 Termination running Distance 33.96 Heading 15.2 deg\n",
      "Episode 1 Step 36 Reward -51.596 Termination running Distance 33.77 Heading 15.2 deg\n",
      "Episode 1 Step 37 Reward -51.237 Termination running Distance 33.63 Heading 15.2 deg\n",
      "Episode 1 Step 38 Reward -50.964 Termination running Distance 33.52 Heading 15.2 deg\n",
      "Episode 1 Step 39 Reward -50.761 Termination running Distance 33.44 Heading 15.2 deg\n",
      "Episode 1 Step 40 Reward -50.611 Termination running Distance 33.38 Heading 15.2 deg\n",
      "Episode 1 Step 41 Reward -50.496 Termination running Distance 33.33 Heading 15.2 deg\n",
      "Episode 1 Step 42 Reward -50.393 Termination running Distance 33.29 Heading 15.2 deg\n",
      "Episode 1 Step 43 Reward -50.314 Termination running Distance 33.26 Heading 15.2 deg\n",
      "Episode 1 Step 44 Reward -50.270 Termination running Distance 33.24 Heading 15.2 deg\n",
      "Episode 1 Step 45 Reward -50.221 Termination running Distance 33.22 Heading 15.2 deg\n",
      "Episode 1 Step 46 Reward -50.191 Termination running Distance 33.21 Heading 15.2 deg\n",
      "Episode 1 Step 47 Reward -50.176 Termination running Distance 33.20 Heading 15.2 deg\n",
      "Episode 1 Step 48 Reward -50.161 Termination running Distance 33.19 Heading 15.2 deg\n",
      "Episode 1 Step 49 Reward -50.145 Termination running Distance 33.19 Heading 15.2 deg\n",
      "Episode 1 Step 50 Reward -50.127 Termination running Distance 33.18 Heading 15.3 deg\n",
      "Episode 1 Step 51 Reward -50.120 Termination running Distance 33.17 Heading 15.3 deg\n",
      "Episode 1 Step 52 Reward -50.091 Termination running Distance 33.17 Heading 15.3 deg\n",
      "Episode 1 Step 53 Reward -50.089 Termination running Distance 33.17 Heading 15.3 deg\n",
      "Episode 1 Step 54 Reward -50.091 Termination running Distance 33.17 Heading 15.3 deg\n",
      "Episode 1 Step 55 Reward -50.084 Termination running Distance 33.16 Heading 15.3 deg\n",
      "Episode 1 Step 56 Reward -50.089 Termination running Distance 33.16 Heading 15.3 deg\n",
      "Episode 1 Step 57 Reward -50.077 Termination running Distance 33.16 Heading 15.3 deg\n",
      "Episode 1 Step 58 Reward -50.078 Termination running Distance 33.16 Heading 15.3 deg\n",
      "Episode 1 Step 59 Reward -50.079 Termination running Distance 33.16 Heading 15.3 deg\n",
      "Episode 1 Step 60 Reward -50.079 Termination running Distance 33.16 Heading 15.3 deg\n",
      "Episode 1 Step 61 Reward -50.095 Termination running Distance 33.16 Heading 15.3 deg\n",
      "Episode 1 Step 62 Reward -50.076 Termination running Distance 33.16 Heading 15.3 deg\n",
      "Episode 1 Step 63 Reward -50.072 Termination running Distance 33.15 Heading 15.3 deg\n",
      "Episode 1 Step 64 Reward -50.070 Termination running Distance 33.15 Heading 15.3 deg\n",
      "Episode 1 Step 65 Reward -50.069 Termination running Distance 33.15 Heading 15.3 deg\n",
      "Episode 1 Step 66 Reward -50.061 Termination running Distance 33.15 Heading 15.3 deg\n",
      "Episode 1 Step 67 Reward -50.069 Termination running Distance 33.15 Heading 15.3 deg\n",
      "Episode 1 Step 68 Reward -50.071 Termination running Distance 33.14 Heading 15.3 deg\n",
      "Episode 1 Step 69 Reward -50.060 Termination running Distance 33.14 Heading 15.3 deg\n",
      "Episode 1 Step 70 Reward -50.046 Termination running Distance 33.13 Heading 15.3 deg\n",
      "Episode 1 Step 71 Reward -50.047 Termination running Distance 33.13 Heading 15.3 deg\n",
      "Episode 1 Step 72 Reward -50.034 Termination running Distance 33.13 Heading 15.3 deg\n",
      "Episode 1 Step 73 Reward -50.030 Termination running Distance 33.12 Heading 15.3 deg\n",
      "Episode 1 Step 74 Reward -50.022 Termination running Distance 33.12 Heading 15.3 deg\n",
      "Episode 1 Step 75 Reward -50.022 Termination running Distance 33.12 Heading 15.3 deg\n",
      "Episode 1 Step 76 Reward -50.014 Termination running Distance 33.11 Heading 15.3 deg\n",
      "Episode 1 Step 77 Reward -50.012 Termination running Distance 33.11 Heading 15.3 deg\n",
      "Episode 1 Step 78 Reward -50.004 Termination running Distance 33.11 Heading 15.3 deg\n",
      "Episode 1 Step 79 Reward -49.994 Termination running Distance 33.10 Heading 15.3 deg\n",
      "Episode 1 Step 80 Reward -49.990 Termination running Distance 33.10 Heading 15.3 deg\n",
      "Episode 1 Step 81 Reward -49.989 Termination running Distance 33.10 Heading 15.3 deg\n",
      "Episode 1 Step 82 Reward -49.983 Termination running Distance 33.10 Heading 15.3 deg\n",
      "Episode 1 Step 83 Reward -49.986 Termination running Distance 33.10 Heading 15.3 deg\n",
      "Episode 1 Step 84 Reward -49.984 Termination running Distance 33.10 Heading 15.3 deg\n",
      "Episode 1 Step 85 Reward -49.989 Termination running Distance 33.10 Heading 15.3 deg\n",
      "\n",
      "Episode 1 Step 1 Reward -52.958 Termination running Distance 35.08 Heading 15.2 deg\n",
      "Episode 1 Step 2 Reward -52.957 Termination running Distance 35.08 Heading 15.2 deg\n",
      "Episode 1 Step 3 Reward -52.959 Termination running Distance 35.08 Heading 15.2 deg\n",
      "Episode 1 Step 4 Reward -52.972 Termination running Distance 35.08 Heading 15.2 deg\n",
      "Episode 1 Step 5 Reward -52.975 Termination running Distance 35.09 Heading 15.2 deg\n",
      "Episode 1 Step 6 Reward -52.975 Termination running Distance 35.09 Heading 15.2 deg\n",
      "Episode 1 Step 7 Reward -52.974 Termination running Distance 35.09 Heading 15.2 deg\n",
      "Episode 1 Step 8 Reward -52.977 Termination running Distance 35.09 Heading 15.2 deg\n",
      "Episode 1 Step 9 Reward -52.977 Termination running Distance 35.09 Heading 15.2 deg\n",
      "Episode 1 Step 10 Reward -52.967 Termination running Distance 35.09 Heading 15.2 deg\n",
      "Episode 1 Step 11 Reward -52.971 Termination running Distance 35.09 Heading 15.2 deg\n",
      "Episode 1 Step 12 Reward -52.978 Termination running Distance 35.09 Heading 15.2 deg\n",
      "Episode 1 Step 13 Reward -52.985 Termination running Distance 35.09 Heading 15.2 deg\n",
      "Episode 1 Step 14 Reward -52.979 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 15 Reward -52.981 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 16 Reward -52.984 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 17 Reward -52.985 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 18 Reward -52.988 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 19 Reward -52.988 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 20 Reward -52.984 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 21 Reward -52.985 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 22 Reward -52.990 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 23 Reward -52.993 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 24 Reward -53.004 Termination running Distance 35.10 Heading 15.2 deg\n",
      "Episode 1 Step 25 Reward -53.004 Termination running Distance 35.09 Heading 15.2 deg\n",
      "Episode 1 Step 26 Reward -53.012 Termination running Distance 35.06 Heading 15.2 deg\n",
      "Episode 1 Step 27 Reward -52.999 Termination running Distance 35.01 Heading 15.2 deg\n",
      "Episode 1 Step 28 Reward -52.958 Termination running Distance 34.95 Heading 15.2 deg\n",
      "Episode 1 Step 29 Reward -52.898 Termination running Distance 34.86 Heading 15.2 deg\n",
      "Episode 1 Step 30 Reward -52.801 Termination running Distance 34.76 Heading 15.2 deg\n",
      "Episode 1 Step 31 Reward -52.679 Termination running Distance 34.63 Heading 15.2 deg\n",
      "Episode 1 Step 32 Reward -52.519 Termination running Distance 34.49 Heading 15.2 deg\n",
      "Episode 1 Step 33 Reward -52.334 Termination running Distance 34.33 Heading 15.2 deg\n",
      "Episode 1 Step 34 Reward -52.119 Termination running Distance 34.15 Heading 15.2 deg\n",
      "Episode 1 Step 35 Reward -51.885 Termination running Distance 33.96 Heading 15.2 deg\n",
      "Episode 1 Step 36 Reward -51.596 Termination running Distance 33.77 Heading 15.2 deg\n",
      "Episode 1 Step 37 Reward -51.237 Termination running Distance 33.63 Heading 15.2 deg\n",
      "Episode 1 Step 38 Reward -50.964 Termination running Distance 33.52 Heading 15.2 deg\n",
      "Episode 1 Step 39 Reward -50.761 Termination running Distance 33.44 Heading 15.2 deg\n",
      "Episode 1 Step 40 Reward -50.611 Termination running Distance 33.38 Heading 15.2 deg\n",
      "Episode 1 Step 41 Reward -50.496 Termination running Distance 33.33 Heading 15.2 deg\n",
      "Episode 1 Step 42 Reward -50.393 Termination running Distance 33.29 Heading 15.2 deg\n",
      "Episode 1 Step 43 Reward -50.314 Termination running Distance 33.26 Heading 15.2 deg\n",
      "Episode 1 Step 44 Reward -50.270 Termination running Distance 33.24 Heading 15.2 deg\n",
      "Episode 1 Step 45 Reward -50.221 Termination running Distance 33.22 Heading 15.2 deg\n",
      "Episode 1 Step 46 Reward -50.191 Termination running Distance 33.21 Heading 15.2 deg\n",
      "Episode 1 Step 47 Reward -50.176 Termination running Distance 33.20 Heading 15.2 deg\n",
      "Episode 1 Step 48 Reward -50.161 Termination running Distance 33.19 Heading 15.2 deg\n",
      "Episode 1 Step 49 Reward -50.145 Termination running Distance 33.19 Heading 15.2 deg\n",
      "Episode 1 Step 50 Reward -50.127 Termination running Distance 33.18 Heading 15.3 deg\n",
      "Episode 1 Step 51 Reward -50.120 Termination running Distance 33.17 Heading 15.3 deg\n",
      "Episode 1 Step 52 Reward -50.091 Termination running Distance 33.17 Heading 15.3 deg\n",
      "Episode 1 Step 53 Reward -50.089 Termination running Distance 33.17 Heading 15.3 deg\n",
      "Episode 1 Step 54 Reward -50.091 Termination running Distance 33.17 Heading 15.3 deg\n",
      "Episode 1 Step 55 Reward -50.084 Termination running Distance 33.16 Heading 15.3 deg\n",
      "Episode 1 Step 56 Reward -50.089 Termination running Distance 33.16 Heading 15.3 deg\n",
      "Episode 1 Step 57 Reward -50.077 Termination running Distance 33.16 Heading 15.3 deg\n",
      "Episode 1 Step 58 Reward -50.078 Termination running Distance 33.16 Heading 15.3 deg\n",
      "Episode 1 Step 59 Reward -50.079 Termination running Distance 33.16 Heading 15.3 deg\n",
      "Episode 1 Step 60 Reward -50.079 Termination running Distance 33.16 Heading 15.3 deg\n",
      "Episode 1 Step 61 Reward -50.095 Termination running Distance 33.16 Heading 15.3 deg\n",
      "Episode 1 Step 62 Reward -50.076 Termination running Distance 33.16 Heading 15.3 deg\n",
      "Episode 1 Step 63 Reward -50.072 Termination running Distance 33.15 Heading 15.3 deg\n",
      "Episode 1 Step 64 Reward -50.070 Termination running Distance 33.15 Heading 15.3 deg\n",
      "Episode 1 Step 65 Reward -50.069 Termination running Distance 33.15 Heading 15.3 deg\n",
      "Episode 1 Step 66 Reward -50.061 Termination running Distance 33.15 Heading 15.3 deg\n",
      "Episode 1 Step 67 Reward -50.069 Termination running Distance 33.15 Heading 15.3 deg\n",
      "Episode 1 Step 68 Reward -50.071 Termination running Distance 33.14 Heading 15.3 deg\n",
      "Episode 1 Step 69 Reward -50.060 Termination running Distance 33.14 Heading 15.3 deg\n",
      "Episode 1 Step 70 Reward -50.046 Termination running Distance 33.13 Heading 15.3 deg\n",
      "Episode 1 Step 71 Reward -50.047 Termination running Distance 33.13 Heading 15.3 deg\n",
      "Episode 1 Step 72 Reward -50.034 Termination running Distance 33.13 Heading 15.3 deg\n",
      "Episode 1 Step 73 Reward -50.030 Termination running Distance 33.12 Heading 15.3 deg\n",
      "Episode 1 Step 74 Reward -50.022 Termination running Distance 33.12 Heading 15.3 deg\n",
      "Episode 1 Step 75 Reward -50.022 Termination running Distance 33.12 Heading 15.3 deg\n",
      "Episode 1 Step 76 Reward -50.014 Termination running Distance 33.11 Heading 15.3 deg\n",
      "Episode 1 Step 77 Reward -50.012 Termination running Distance 33.11 Heading 15.3 deg\n",
      "Episode 1 Step 78 Reward -50.004 Termination running Distance 33.11 Heading 15.3 deg\n",
      "Episode 1 Step 79 Reward -49.994 Termination running Distance 33.10 Heading 15.3 deg\n",
      "Episode 1 Step 80 Reward -49.990 Termination running Distance 33.10 Heading 15.3 deg\n",
      "Episode 1 Step 81 Reward -49.989 Termination running Distance 33.10 Heading 15.3 deg\n",
      "Episode 1 Step 82 Reward -49.983 Termination running Distance 33.10 Heading 15.3 deg\n",
      "Episode 1 Step 83 Reward -49.986 Termination running Distance 33.10 Heading 15.3 deg\n",
      "Episode 1 Step 84 Reward -49.984 Termination running Distance 33.10 Heading 15.3 deg\n",
      "Episode 1 Step 85 Reward -49.989 Termination running Distance 33.10 Heading 15.3 deg\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['conda', 'run', '-n', 'parking-rl', 'python', 'main.py', '--mode', 'manual', '--episodes', '1', '--max-steps', '4000', '--config', '/home/ansatz/ME5418/parking_project/generated_configs/train_001.json'], returncode=0)"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "project_root = Path(\".\").resolve()\n",
    "cmd = [\n",
    "    \"conda\", \"run\", \"-n\", \"parking-rl\", \"python\", \"main.py\",\n",
    "    \"--mode\", mode,\n",
    "    \"--episodes\", str(episodes),\n",
    "    \"--max-steps\", str(max_steps),\n",
    "]\n",
    "\n",
    "if sleep_scale != 0.5:\n",
    "    cmd.extend([\"--sleep-scale\", str(sleep_scale)])\n",
    "\n",
    "if config_path is not None:\n",
    "    if write_notebook_config:\n",
    "        config_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with config_path.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "            json.dump(custom_config, fh, indent=2, ensure_ascii=False)\n",
    "            fh.write(\"\\n\")\n",
    "    cmd.extend([\"--config\", str(config_path.resolve())])\n",
    "\n",
    "print(\"Launching CLI demo:\", \" \".join(cmd))\n",
    "env_vars = os.environ.copy()\n",
    "env_vars[\"MPLBACKEND\"] = \"QtAgg\"\n",
    "subprocess.run(cmd, cwd=project_root, env=env_vars, check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dde97c",
   "metadata": {},
   "source": [
    "## 训练接入说明（中文说明）\n",
    "\n",
    "- 在训练脚本中可以 `env = gym.make(\"ParkingEnv-v0\", config=custom_cfg)`，或从 `parking_gym import ParkingEnv` 自行实例化；省略 `config` 时会采用默认参数（包含 9 束雷达）。\n",
    "- 观测向量长度为 `11 + len(ray_angles)`，可通过 `env.observation_space.shape[0]` 或一次 `obs, _ = env.reset()` 验证；顺序与上文状态列表一致，末尾为 0~1 归一化的激光距离。\n",
    "- `env.step(action)` 需要形如 `np.array([lon_accel, steer_accel], dtype=np.float32)` 的二维动作，可参考下方示例。\n",
    "- 若需关闭观测噪声以便评估，可调用 `env.unwrapped.set_observation_noise(enabled=False)`；重新开启或调整标准差时通过 `std` 参数指定。\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from parking_gym import ParkingEnv\n",
    "\n",
    "env = ParkingEnv()  # 或 gym.make(\"ParkingEnv-v0\")\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.array([0.0, 0.0], dtype=np.float32)  # 替换为策略输出\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e99629",
   "metadata": {},
   "source": [
    "## Training Integration Notes (English Explanation)\n",
    "\n",
    "- In your training script call `env = gym.make(\"ParkingEnv-v0\", config=custom_cfg)` or import `ParkingEnv` directly; omitting `config` uses the synced defaults (with nine lidar beams).\n",
    "- The observation vector length is `11 + len(ray_angles)`; confirm with `env.observation_space.shape[0]` or by running `obs, _ = env.reset()`. The order matches the earlier state list, with the lidar distances occupying the tail.\n",
    "- `env.step(action)` expects a two-dimensional action such as `np.array([lon_accel, steer_accel], dtype=np.float32)`; see the snippet below.\n",
    "- Disable observation noise during evaluation via `env.unwrapped.set_observation_noise(enabled=False)`, or adjust the standard deviation by passing a `std` value.\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from parking_gym import ParkingEnv\n",
    "\n",
    "env = ParkingEnv()  # or gym.make(\"ParkingEnv-v0\")\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.array([0.0, 0.0], dtype=np.float32)  # replace with policy output\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parking-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
